# 索引的基础知识

## 为何引入B+树

影响查询数据快慢的因素

1. 查询的数据总量较大  而数据拥有多少数据并不是数据库本身能够决定的  :x:
2. 查询的次数较多 而数据库本身可以优化数据结构来优化查询次数 :white_check_mark:



二分查找/hash表(hash冲突之后会导致查询效率下降) 

:arrow_down:

二叉树(无序也不太行)

:arrow_down:

二叉排序树(如果插入数据顺序排列,就会导致树的高度太高)

:arrow_down:

平衡二叉树(用插入的成本来弥补查询的效率,插入情况特别多的话,要保持平衡需要更多的消耗)

:arrow_down:

红黑树[最长子树不超过最短子树的两倍即可,更多特性,为了解决AVL的过度平衡问题] (数据量过大也会使树的深度过大,我们需要跳脱出二叉树的想法)

:arrow_down:

B树(由于数据存储在每个结点上，假设每次IO取16kb的数据，这样使得我们每个结点保存的索引会更少，为了让每次IO读取的索引尽可能的多，引入B+树)

:arrow_down:

B+树(除叶子结点外，其他的结点只存储指针和索引值，这样可以存储更多的索引，每次IO能够查找更多的结点)



所以对于主键索引，InnoDB 基于 b+ 树，会把索引值存在非叶子结点，数据全部存在叶子结点上。
对于非主键索引，其他索引的叶子结点只存储主键值，再通过主键值来找到数据。

所以基于B+树的这种特性，IO次数在两三次的时候就能知道大概几千万到几亿的数据量，这种效率是非常高的。而表结构的数量最好也不要超过这个数字。

---

## B树与B+树的区别

B树：满足下列要求的m叉树

1. 树中每个结点至多有m个孩子结点(即至多有m-1个关键字)
2. 每个结点的结构为: n,p0,k1,p1,k2,...,kn,pn p为指向子树的指针,k为关键字,n代表有多少个结点
3. 除根结点外,其他结点至少有m/2个孩子结点
4. 若根节点不是叶子结点,则根节点至少有两个孩子结点
5. 所有叶子结点都在同一层上,即B树是所有结点的平衡因子均等于0的多路查找树

B树（B-tree）和B+树（B+ tree）是两种常见的数据结构，用于实现在磁盘或其他大容量存储设备上的索引结构，常用于数据库系统中。它们之间的主要区别在于其内部节点和叶子节点的结构以及搜索方式。下面是它们的主要区别：

1. **节点结构**：
   - **B树**：B树的每个节点既可以是内部节点，也可以是叶子节点。内部节点存储键值对和子节点的指针，叶子节点存储键值对和相关数据的指针。
   - **B+树**：B+树的内部节点只存储键值对，不存储数据指针，所有的数据都存储在叶子节点中。叶子节点通过指针连接形成链表。

2. **叶子节点**：
   - **B树**：在B树中，叶子节点存储了实际的数据，叶子节点之间通过指针连接。
   - **B+树**：B+树的叶子节点只存储键值对，数据存储在叶子节点中形成的链表中，叶子节点之间通过指针连接形成了一个有序链表。

3. **搜索方式**：
   - **B树**：在B树中，搜索可以从根节点开始，逐级向下查找，如果在内部节点中找到了键值，则根据键值指示的指针进入下一级节点，直到找到目标数据或者叶子节点。
   - **B+树**：B+树的搜索也是从根节点开始，但只有到达叶子节点时才找到目标数据，因为所有的数据都存储在叶子节点中。

4. **范围查询性能**：
   - **B树**：由于B树的叶子节点包含了数据，范围查询时需要在叶子节点之间进行遍历，性能可能不如B+树。
   - **B+树**：B+树的叶子节点形成了一个有序链表，范围查询时只需要在链表上进行遍历，性能更高。

总的来说，B+树相较于B树在范围查询上具有更好的性能，适用于大多数需要在磁盘或其他大容量存储设备上进行索引的场景。而B树在某些特定场景下可能会有一些优势，比如需要频繁地执行范围查询和局部查询的情况。

下面是一个简单的示意图，分别展示了B树和B+树的结构：

```
B树示意图：

          [20, 50]
         /    |    \
     [5,10] [30] [60, 70]
     /  |  \          |
 [2,3] [6] [15]       [65, 80]

B+树示意图：

          [20, 50]
         /    |    \
      [5]  [30]  [60]
      / \     |    | \
    [2,3][6,15]  |  [65, 80]
                 |
                [70]
```

在上面的示意图中，方框表示节点，方框内的数字表示节点存储的键值对。箭头表示节点之间的指针连接。在B树中，每个节点既可以是内部节点，也可以是叶子节点；而在B+树中，内部节点只存储键值对，所有的数据都存储在叶子节点中形成的链表中。

---

## 聚簇索引和非聚簇索引

### 📌 1. **概念**

#### ✅ 聚簇索引（Clustered Index）

- 数据表的 **数据行的物理顺序** 与索引的顺序 **一致**。
- 一个表 **只能有一个聚簇索引**（因为数据行只能有一种物理顺序）。
- 在大多数关系型数据库（如 MySQL InnoDB）中，**主键默认就是聚簇索引**。

#### ✅ 非聚簇索引（Non-Clustered Index）

- 数据表的物理顺序和索引顺序 **无关**。
- 索引里保存的是键值和对应数据行的指针（如行号或聚簇索引的键）。
- 一个表可以有 **多个非聚簇索引**。

------

### 📌 2. **示例**

以 MySQL InnoDB 为例：

```sql
CREATE TABLE users (
  id INT PRIMARY KEY,  -- id 就是聚簇索引
  name VARCHAR(50),
  age INT
);

CREATE INDEX idx_name ON users(name);  -- name 是非聚簇索引
```

------

### 📌 3. **区别**

| 项目     | 聚簇索引                           | 非聚簇索引                   |
| -------- | ---------------------------------- | ---------------------------- |
| 数据存储 | 数据和索引一起存储                 | 索引和数据分开存储           |
| 顺序     | 数据物理顺序与索引顺序一致         | 数据物理顺序与索引顺序无关   |
| 数量     | 一个表只能有一个                   | 可以有多个                   |
| 查询效率 | 范围查询快（因为数据物理顺序一致） | 查找需要回表（除非覆盖索引） |
| 维护成本 | 插入/更新时可能需要移动数据行      | 维护相对简单                 |

------

### 📌 4. **优缺点**

#### ✅ 聚簇索引

**优点**：

- 范围查询快，如 `BETWEEN`、`ORDER BY`。
- 访问索引即访问数据，无需回表。

**缺点**：

- 插入新数据时，如果插入点在中间，可能需要移动大量数据。
- 只能有一个。

------

#### ✅ 非聚簇索引

**优点**：

- 可以有多个，灵活支持多种查询条件。
- 只需维护键值和指针，不影响物理存储。

**缺点**：

- 需要回表（除非覆盖索引）。
- 对于大数据量随机查询，性能比聚簇索引略低。

------

### 📌 5. **使用场景**

- **聚簇索引**：适合主键、唯一且经常用于范围查询的列。
- **非聚簇索引**：适合频繁作为过滤条件的列，如用户名、状态、类别等。

------

# 联合索引

## 📌 1️⃣ 联合索引是如何存储的？

MySQL 的联合索引（组合索引），其实是 **多列值按顺序拼接** 后，存储在同一个 B+ 树中的。

举个例子：

```sql
CREATE TABLE user (
  id INT PRIMARY KEY,
  age INT,
  gender CHAR(1),
  city VARCHAR(20),
  KEY idx_age_gender_city (age, gender, city)
);
```

这个 `idx_age_gender_city` 的底层 B+ 树存储的每个索引项，就像：

```
(age, gender, city) => 主键值
```

索引项按 **先 age、再 gender、再 city** 的字典序排列，内部结构是：

```
(age1, gender1, city1)  
(age1, gender1, city2)  
(age1, gender2, city1)  
(age2, gender1, city1)
...
```

所以本质是 **多列值拼成一个复合排序键**。

------

## 📌 2️⃣ 为什么联合索引顺序很重要？

因为 B+ 树的顺序决定了能否快速定位范围：

- 按索引第 1 列（age）查，一定能命中索引；
- 按索引前 2 列（age, gender）查，也能精准命中；
- 按索引第 3 列（city）单独查，一般用不上（除非 `city` 前面两个条件是常量，或者用到索引覆盖）。

这就是所谓的 **最左前缀原则（Leftmost Prefix Rule）**。

------

## 📌 3️⃣ 什么是最左前缀匹配？

**MySQL 联合索引只能从最左列开始连续使用。**

- ✅ 能用到索引的情况：
  - `WHERE age = 30`
  - `WHERE age = 30 AND gender = 'M'`
  - `WHERE age = 30 AND gender = 'M' AND city = 'Beijing'`
  - `WHERE age = 30 AND city = 'Beijing'`（只要前缀连续，中间不跳列，这个其实没用到 gender）
- ❌ 用不到索引：
  - `WHERE gender = 'M'` （跳过了 age，最左列不匹配）
  - `WHERE city = 'Beijing'`（跳了前两列）
  - `WHERE gender = 'M' AND city = 'Beijing'`（跳了 age）

------

## 📌 4️⃣ 最左前缀还支持范围查询，但有坑

如果中间列是范围，会截断：

例如：

```sql
WHERE age = 30 AND gender > 'M' AND city = 'Beijing'
```

这里：

- age = 30：命中
- gender > 'M'：是范围
- city = 'Beijing'：**会失效**

因为一旦范围（不等于、>、<、BETWEEN、LIKE 'abc%'）出现，后面的列就不能继续用于范围扫描了。

------

## 📌 5️⃣ 如何最大化发挥联合索引作用？

👉 **小结一条黄金原则：**
 **WHERE 中匹配列的顺序，要尽量连续覆盖联合索引的最左前缀，且把区分度高的列放前面。**

🔑 小技巧：

- 等值匹配列尽量放在前面，范围匹配列放后面。
- 如果有 ORDER BY，用上索引列的顺序可以避免 filesort。

------

## 📌 6️⃣ EXPLAIN 如何判断是否命中联合索引？

可以用：

```sql
EXPLAIN SELECT * FROM user WHERE age = 30 AND gender = 'M';
```

看 `key` 字段是 `idx_age_gender_city`，`rows` 很小，并且 `Extra` 没有 `Using where` 或者 `Using filesort`，说明利用得很好。

------

## 📌 7️⃣ 如果 SELECT 只读索引覆盖到的列，还能避免回表

比如：

```sql
SELECT age, gender FROM user WHERE age = 30 AND gender = 'M';
```

因为 `age` 和 `gender` 都在索引里，InnoDB 可以直接用索引叶子节点返回数据，不用回表查主键索引。这就是 **覆盖索引（Covering Index）**。

------

## 📌 8️⃣ 小结

✅ **联合索引本质：**
 多列值按顺序组成一个联合排序键，存储在 B+ 树里。

✅ **生效关键：**
 查询时条件要匹配最左前缀，并且尽量连续。

✅ **优化点：**
 设计索引时把高区分度列放前面、常用过滤列放前面；写 SQL 时要遵循最左匹配原则。

---

# 覆盖索引 (Covering Index)

覆盖索引是指一个索引包含了查询所需的所有字段，使得查询可以直接从索引中获取数据而无需回表查询数据行。

## 核心概念

1. **无需回表**：当查询的所有列都包含在索引中时，数据库引擎可以直接使用索引数据返回结果，而不需要再去查找主表。
2. **性能优势**：减少了I/O操作，因为不需要访问数据行，只需读取索引数据。

## 工作原理

常规索引查询流程：

```
索引查找 → 获取行位置 → 回表获取完整数据行
```

覆盖索引查询流程：

```
索引查找 → 直接返回索引中的数据（无需回表）
```

## 创建覆盖索引示例

假设有一个用户表：

```
CREATE TABLE users (
    id INT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at TIMESTAMP
);
```

如果要优化以下查询：

```
SELECT username, email FROM users WHERE username = 'john';
```

可以创建覆盖索引：

```
CREATE INDEX idx_username_email ON users(username, email);
```

## 优点

1. 显著提高查询性能
2. 减少I/O操作
3. 减少内存使用（只需缓存索引数据）
4. 对于InnoDB等存储引擎特别有效

## 注意事项

1. 索引会占用额外存储空间
2. 写入操作（INSERT/UPDATE/DELETE）会有额外开销
3. 不是所有查询都适合使用覆盖索引

---

# InnoDB范围查询支持索引吗

**InnoDB 支持在范围查询（Range Query）中使用索引**，但具体效果取决于查询条件和索引结构。

------

## 1. **范围查询与索引的关系**

- **支持情况**：InnoDB 的 B+ 树索引（默认索引类型）对范围查询（如 `WHERE column BETWEEN x AND y`、`WHERE column > value` 等）是支持的，但**范围查询之后的列无法继续利用索引**（即索引的“最左前缀原则”会中断）。

- 示例：

  ```
  -- 假设有联合索引 (a, b, c)
  SELECT * FROM table WHERE a = 1 AND b > 2 AND c = 3;
  ```

  - 索引会用到 `a` 和 `b`（因为 `a` 是等值查询，`b` 是范围查询），但 `c` 无法利用索引（因为 `b` 已经是范围查询，B+ 树无法继续精确匹配后续列）。

------

## 2. **范围查询的性能特点**

- **索引范围扫描**：InnoDB 会通过索引定位到范围起始点，然后顺序扫描索引直到范围结束（类似“跳跃式”访问），比全表扫描高效。
- **回表问题**：如果查询的列不在索引中（非覆盖索引），需要通过主键回表查询完整数据，可能增加 I/O 开销。

------

## 3. **优化建议**

- 尽量让范围查询的列靠后：在联合索引中，将等值查询的列放在范围查询列之前，以最大化索引利用率。

  ```
  -- 更优的索引设计（a 是等值查询，b 是范围查询）
  CREATE INDEX idx ON table(a, b);
  ```

- **避免大范围查询**：范围过大可能导致扫描大量索引页，此时全表扫描可能更高效（可通过 `EXPLAIN` 分析）。

- **覆盖索引**：如果查询的列全部在索引中（如 `SELECT a, b FROM table WHERE a > 1`），可避免回表，提升性能。

------

## 4. **与 `EXPLAIN` 验证**

使用 `EXPLAIN` 查看执行计划，确认是否使用了索引（`type` 列显示 `range` 表示范围扫描）：

```
EXPLAIN SELECT * FROM table WHERE a > 1;
```

------

## Tips

范围查询范围过大的索引扫描成本可能高于全表扫描导致：⚠️ 索引可能失效（优化器可能选择全表扫描）

---

# 为什么要遵守最左前缀原则

## 复合索引结构

B+树索引是一种多路搜索树，用于支持高效的数据检索操作。

假设我们有一个复合索引（a, b, c），现在来绘制一个包含一些示例数据的B+树索引结构。

首先，让我们考虑以下示例数据：

- (a=10, b=20, c=30)
- (a=10, b=25, c=35)
- (a=15, b=30, c=40)
- (a=20, b=40, c=50)

在B+树索引中，节点分为内部节点和叶子节点。内部节点存储键值范围和指向子节点的指针，而叶子节点存储实际的索引键值和指向数据行的引用或指针。

下面是一个简化的B+树示意图：

```
                             +----------------------------+
                             |             (10,*)         |
                             +----------------------------+
                            /                              \
        +---------------+                                  +---------------+
        | (10,*)        |                                  | (15,*)        |
        +---------------+                                  +---------------+
       /                  \                                /                \
+---------------+   +---------------+              +---------------+   +---------------+
| (10,20,30)    |   | (10,25,35)    |              | (15,30,40)    |   | (20,40,50)    |
+---------------+   +---------------+              +---------------+   +---------------+
```

- 每个内部节点包含了键值范围和指向子节点的指针。例如，第一个内部节点包含了键值范围 (10, *)，表示大于等于10的所有数据。
- 叶子节点存储了实际的索引键值和指向数据行的引用。例如，叶子节点包含了键值 (10, 20, 30)，表示具有a=10、b=20和c=30的数据行。

------

## **不遵守最左前缀原则的后果**

### **(1) 索引失效，退化为全表扫描**

- 如果查询条件不包含最左列（如 `WHERE b = 2 AND c = 3`），MySQL 无法利用 `(a, b, c)` 索引，可能直接扫描全表。

- 

  示例

  ：

  ```
  -- 假设索引是 (a, b, c)
  SELECT * FROM table WHERE b = 2 AND c = 3;  -- 索引失效，全表扫描
  ```

### **(2) 部分索引失效，仅能利用部分列**

- 如果范围查询出现在联合索引的中间列（如 `b > 2`），则 **`b` 及其右侧的列（如 `c`）无法利用索引**。

- 

  示例

  ：

  ```
  -- 假设索引是 (a, b, c)
  SELECT * FROM table WHERE a = 1 AND b > 2 AND c = 3;  -- 只能用到 a 和 b，c 无法利用
  ```

# order by 为什么会导致索引失效

这个问题很关键：**`ORDER BY` 并不会直接让索引“失效”，而是因为它要求的有序性超出了索引的天然顺序能力**。

------

## 🎯 **核心点：索引能否直接提供你想要的顺序**

MySQL 的 B+ 树索引有两个核心限制：
 1️⃣ **顺序是固定的** —— 索引是按定义时的列顺序和方向（ASC）排的
 2️⃣ **只能顺着树扫描** —— 只能用“从小到大”或“从大到小”的顺序来遍历

一旦你的 `ORDER BY` 和这个顺序 **冲突**，就没法沿着树顺序扫描了，只能走完索引后再做二次排序（`filesort`）。

------

## ✅ **看例子就懂了**

------

### 📌 1️⃣ 顺序方向不一致

```sql
-- 索引：
CREATE INDEX idx_ab (a ASC, b ASC);

-- 查询：
SELECT * FROM t ORDER BY a ASC, b DESC;
```

索引里的顺序是 `(a ASC, b ASC)` 你要 `(a ASC, b DESC)` 对同一个 `a`，索引里的 `b` 已经是升序了，但你想要降序。

所以：

- MySQL 没办法“在树里顺序走”就得到 `b DESC`
- 只能把匹配结果读出来后，用 `filesort` 排一次

------

### 📌 2️⃣ 排序列不是连续前缀

```sql
-- 索引：
CREATE INDEX idx_abc (a, b, c);

-- 查询：
SELECT * FROM t WHERE a = 1 ORDER BY c;
```

这里：

- 索引顺序是 `(a, b, c)`
- 你 `WHERE a=1` 没问题，但 `ORDER BY c` 跳过了 `b`
- 跳过的那列可能打乱了 `c` 的顺序，因为 `b` 在 `a` 相同的分组里才排好顺序

所以：

- 没法用索引顺序，只能全取出来再排序

------

### 📌 3️⃣ 不等号 & 范围查询

```sql
-- 索引：
CREATE INDEX idx_ab (a, b);

-- 查询：
SELECT * FROM t WHERE a > 10 ORDER BY b;
```

- 范围条件 `a > 10` 会用到 `a` 索引部分
- 但 `b` 对应的顺序不能保证连续，因为范围扫描只保证了 `a` 的范围

所以：

- `b` 排序失效，得二次排序

------

### 📌 4️⃣ 索引顺序 OK，但 LIMIT 太大

这不是索引失效，而是没效率：

```sql
SELECT * FROM big_table ORDER BY age LIMIT 100000;
```

即使走了索引，回表要读 10 万行，代价也很高。所以大分页也常被误以为“索引没生效”，其实是 IO 太多。

------

## 🔍 **怎么判断？**

`EXPLAIN`：

- `Using index` + 没有 `Using filesort` → 用了索引顺序
- `Using filesort` → 没有完全利用索引排序

------

## ⚙️ **结论**

`ORDER BY` 让索引失效的本质：

- 并不是索引无效，而是索引给不出来你需要的顺序，或者顺序中间被“断开”了。

------



# 索引创建原则

索引是数据库性能优化的关键手段，但滥用索引反而会降低性能（增加写入开销、占用更多存储空间）。以下是 **应该创建索引** 和 **尽量避免创建索引** 的字段类型及场景分析。

------

## **一、应该创建索引的字段**

### **1. WHERE 条件高频过滤的字段**

- **适用场景**：查询条件（`WHERE`）中频繁使用的字段。

- 示例：

  ```
  -- 如果经常按 `user_id` 查询用户数据
  SELECT * FROM users WHERE user_id = 100;
  ```

  - **建议**：在 `user_id` 上创建索引。

### **2. JOIN 连接的字段**

- **适用场景**：多表关联查询时，用于 `JOIN` 的字段。

- 示例：

  ```
  -- 如果经常通过 `order_id` 关联订单表和用户表
  SELECT * FROM orders JOIN users ON orders.user_id = users.user_id;
  ```

  - **建议**：在 `orders.user_id` 和 `users.user_id` 上创建索引。

### **3. ORDER BY / GROUP BY 的字段**

- **适用场景**：排序（`ORDER BY`）或分组（`GROUP BY`）的字段。

- 示例：

  ```
  -- 如果经常按 `create_time` 排序查询订单
  SELECT * FROM orders ORDER BY create_time DESC;
  ```

  - **建议**：在 `create_time` 上创建索引。

### **4. 高选择性字段（唯一值多）**

- **适用场景**：字段的值分布广泛，能快速缩小查询范围。
- 示例：
  - 用户表的 `email`、`手机号`（几乎唯一）。
  - 订单表的 `订单号`（唯一）。
  - **建议**：对高选择性字段创建索引（尤其是唯一索引）。

### **5. 覆盖索引的字段**

- **适用场景**：查询只需要返回索引列，无需回表。

- 示例：

  ```
  -- 如果查询只需要 `status` 和 `create_time`，且这两列有联合索引
  SELECT status, create_time FROM orders WHERE status = 'pending';
  ```

  - **建议**：创建 `(status, create_time)` 联合索引（覆盖索引）。

------

## **二、尽量避免创建索引的字段**

### **1. 低选择性字段（重复值多）**

- **适用场景**：字段的值分布集中，索引效果差。
- 示例：
  - 性别（`male/female`）、布尔值（`true/false`）、状态字段（如 `status=0/1/2`）。
  - **原因**：索引无法有效缩小查询范围（比如 90% 的数据是 `status=1`，索引几乎无用）。
  - **例外**：如果业务必须按低选择性字段查询（如“查询所有女性用户”），可以创建索引，但需结合其他字段（如联合索引 `(gender, age)`）。

### **2. 频繁更新的字段**

- **适用场景**：字段经常被 `UPDATE` 修改。

- **原因**：索引会降低写入性能（每次更新数据需同步更新索引）。

- 示例：

  ```
  -- 如果 `last_login_time` 频繁更新
  UPDATE users SET last_login_time = NOW() WHERE user_id = 100;
  ```

  - **建议**：除非业务必须按此字段查询，否则避免创建索引。

### **3. 大文本字段（如 `TEXT`、`BLOB`）**

- **适用场景**：存储大量数据的字段（如文章内容、图片二进制数据）。
- 原因：
  - 索引会占用大量存储空间。
  - 查询时通常不会用 `WHERE` 直接过滤大文本字段（而是用 `LIKE` 或全文检索）。
- 替代方案：
  - 对大文本字段使用 **全文索引**（如 MySQL 的 `FULLTEXT` 索引）。
  - 避免直接对大字段建普通索引。

### **4. 不参与查询的字段**

- **适用场景**：字段从未在 `WHERE`、`JOIN`、`ORDER BY` 等语句中使用。
- **原因**：索引无任何优化价值，反而增加维护成本。

------

## **三、特殊场景的索引设计建议**

### **1. 高并发写入场景**

- 建议：
  - 减少索引数量（写入性能与索引数量成反比）。
  - 避免在频繁更新的字段上建索引。

### **2. 数据量小的表**

- 建议：
  - 如果表数据量很小（如几千行），全表扫描可能比索引更快（索引查找+回表的开销可能超过全表扫描）。

### **3. 复合索引的设计原则**

- **遵循最左前缀匹配**：将高频查询条件放在联合索引的最左边。
- **避免冗余索引**：如已有 `(a, b)` 索引，再单独建 `a` 索引是多余的。

------

## **四、总结：索引创建的黄金法则**

| **字段类型**               | **是否建议建索引**     | **原因**             |
| -------------------------- | ---------------------- | -------------------- |
| **高频查询条件字段**       | ✅ 建议                 | 快速缩小查询范围     |
| **JOIN 关联字段**          | ✅ 建议                 | 加速多表关联         |
| **ORDER BY/GROUP BY 字段** | ✅ 建议                 | 避免文件排序         |
| **高选择性字段**           | ✅ 建议                 | 唯一值多，索引效率高 |
| **低选择性字段**           | ❌ 避免（除非业务必须） | 索引效果差           |
| **频繁更新字段**           | ❌ 避免                 | 写入性能下降         |
| **大文本字段**             | ❌ 避免                 | 占用空间大，查询少   |
| **不参与查询的字段**       | ❌ 避免                 | 无优化价值           |

### **最终建议**

1. **优先为 `WHERE`、`JOIN`、`ORDER BY` 的字段建索引**。
2. **避免为低选择性、频繁更新的字段建索引**。
3. **使用 `EXPLAIN` 分析查询，验证索引是否生效**。
4. **定期优化索引**（删除冗余索引，调整复合索引顺序）。



# 什么是索引下推

## 📌 1️⃣ 什么是索引下推（ICP）？

**索引下推**，全称 **Index Condition Pushdown**，是一种优化手段，它允许数据库在 **索引遍历阶段** 就对部分 WHERE 条件进行过滤，而不是等到通过索引找到数据行后再回表到数据页进行过滤。

ICP 是从 MySQL 5.6 开始引入的，主要针对 **二级索引**（非聚簇索引）的回表操作。

------

## 📌 2️⃣ ICP 出现前是怎么执行查询的？

传统的索引查询过程（以二级索引为例）：

1️⃣ 扫描索引 B+ 树，找到满足索引条件的索引记录（只包含索引列）。
 2️⃣ 对于满足索引条件的每条记录，去主键索引（聚簇索引）中回表读取整行数据。
 3️⃣ 回表后，再对剩余的 WHERE 条件做进一步判断。

⚡ **问题：** 如果 WHERE 里有的条件是索引里就包含的，但没在索引的索引条件里用上，没办法在索引扫描阶段就过滤，结果就是会读很多行回表，然后发现其实不满足，又丢弃了，造成不必要的 I/O。

------

## 📌 3️⃣ ICP 是怎么优化的？

**ICP 的核心点：**
 在索引扫描阶段，就把 **能在索引中判断的 WHERE 条件** 一起下推到索引遍历过程中，提前过滤掉一部分不符合条件的记录，减少回表次数。

✅ **什么时候能下推？**

- 条件必须只涉及索引中已有的列。
- 条件不能是复杂的需要表数据才能判断的（比如 `WHERE col1 > 10 AND col2 = 'abc'`，如果索引只包含 col1 和 col2，就可以下推）。

------

## 📌 4️⃣ 举个简单的例子

假设：

```sql
CREATE TABLE user (
  id INT PRIMARY KEY,
  age INT,
  gender CHAR(1),
  KEY idx_age_gender (age, gender)
);

-- 查询
SELECT * FROM user WHERE age > 30 AND gender = 'M';
```

- 如果没有 ICP，可能只用到 `age > 30` 来定位索引范围，然后找到符合 `age` 的记录后去回表，再判断 `gender`。
- 有了 ICP，`gender = 'M'` 也能在索引里判断，直接在索引遍历阶段过滤掉不满足的，减少回表行数。

------

## 📌 5️⃣ ICP 的原理简单说就是：

- 解析器将 WHERE 条件拆分为：
  👉 一部分是索引条件（用来确定索引扫描的范围）。
  👉 一部分是可下推条件（在索引遍历时能继续过滤的条件）。
  👉 剩余必须回表才能判断的条件（如有）。

------

## 📌 6️⃣ ICP 的优势

✅ **1. 降低回表次数**
 最直接，回表是随机 I/O，代价比单纯扫描索引页高得多。

✅ **2. 提高查询性能**
 更少的 I/O、更少的行处理，性能提升可观。

✅ **3. 对组合索引特别有效**
 当只使用了索引的部分列来定位范围时，其余列也可以用来过滤。

------

## 📌 7️⃣ ICP 是为了解决什么问题？

ICP 的主要目的是：

> **降低不必要的回表 I/O** —— 在索引扫描阶段尽可能筛掉不符合条件的数据。

ICP 的出现是为了让**存储引擎**在扫描索引时也能做尽可能多的过滤，而不是把所有过滤都留给**SQL执行器**在回表后做。

------

## 📌 8️⃣ 有什么限制吗？

- ICP 只在二级索引上有显著意义，对聚簇索引（主键索引）没用，因为本来就已经在读取数据行了。
- 复杂的表达式（比如函数、OR 连接条件、非索引列条件）不能下推。
- ICP 开启与否是由优化器自动决定，也可以通过 `EXPLAIN` 看执行计划里是否使用了 `Using index condition`。

------

## 📌 总结一句话

> **索引下推 = 在索引层面把能过滤的条件尽量过滤掉，尽可能减少回表，提升查询效率。**

---

# explain+ICP分析

## 📌 1️⃣ EXPLAIN 到底是干啥用的？

`EXPLAIN` 是 MySQL 提供的执行计划分析工具，用来告诉你：

✅ MySQL 是怎么执行这条 SQL 的（比如走哪个索引）
 ✅ 每一步访问了多少行（估算）
 ✅ 是否做了回表
 ✅ 是否使用了 ICP（Index Condition Pushdown）
 ✅ 是否使用了覆盖索引
 ✅ 是否用了 filesort（额外的排序操作）
 ✅ 是否使用了临时表

这就相当于 **把优化器执行 SQL 的思路** 全部摊开在你面前。

------

## 📌 2️⃣ EXPLAIN 输出的核心字段含义

当你执行：

```sql
EXPLAIN SELECT * FROM user WHERE age = 30 AND gender = 'M';
```

会输出一张表，典型字段含义如下：

| 字段            | 含义                                                         |
| --------------- | ------------------------------------------------------------ |
| `id`            | 执行顺序标识，同一个 SQL 里如果有多张表 JOIN，会有多行 ID    |
| `select_type`   | 查询类型（SIMPLE, PRIMARY, SUBQUERY, UNION 等）              |
| `table`         | 访问的表                                                     |
| `partitions`    | 用到的分区（有分区表时才显示）                               |
| `type`          | 连接类型（system, const, eq_ref, ref, range, index, ALL）越靠左性能越好 |
| `possible_keys` | 可能用到的索引                                               |
| `key`           | 实际使用的索引                                               |
| `key_len`       | 实际使用索引长度                                             |
| `ref`           | 哪个列或常数与索引进行比较                                   |
| `rows`          | 估算扫描的行数                                               |
| `filtered`      | 按条件过滤后剩余百分比（近似）                               |
| `Extra`         | 额外信息，ICP、Using index、Using where、Using filesort、Using temporary |

------

## 📌 3️⃣ ICP（索引下推）怎么体现？

看 `Extra` 列：

- 如果看到 `Using index condition`，就是开启了 **ICP**。
  ➜ 表示某些 WHERE 条件已经在索引遍历阶段被过滤掉了。
- 如果只有 `Using where`，则表示是在回表后过滤。
- 如果有 `Using index`，说明用到了 **覆盖索引**（即不需要回表，只靠索引就能返回结果）。

举个示例对比：

**🌟 场景 1：只有索引条件**

```sql
EXPLAIN SELECT * FROM user WHERE age = 30;
```

可能 Extra: `Using where`
 （只用到索引的第一个列，索引只是确定范围，剩下的过滤是回表做）

------

**🌟 场景 2：有 ICP**

```sql
EXPLAIN SELECT * FROM user WHERE age = 30 AND gender = 'M';
```

- 如果 `gender` 在索引里，且没有用来做范围扫描（只是过滤），MySQL 会把 `gender = 'M'` 下推。
- 这时 `Extra` 会出现：`Using index condition`

------

**🌟 场景 3：覆盖索引**

```sql
EXPLAIN SELECT age, gender FROM user WHERE age = 30 AND gender = 'M';
```

- 如果 `SELECT` 的列都在索引里，Extra 会是：

  ```
  Using index condition; Using index
  ```

- `Using index` 表示是覆盖索引（不会回表），`Using index condition` 表示用了 ICP。

------

## ✅ EXPLAIN 演示的实际流程

比如你有下面这张表和索引：

```sql
CREATE TABLE user (
  id INT PRIMARY KEY,
  age INT,
  gender CHAR(1),
  city VARCHAR(20),
  KEY idx_age_gender (age, gender)
);
```

**SQL 1:**

```sql
EXPLAIN SELECT * FROM user WHERE age > 20 AND gender = 'M';
```

- `age > 20`：范围条件

- `gender = 'M'`：ICP

- 结果里 `Extra` 会显示：

  ```
  Using index condition; Using where
  ```

  - `index condition` = gender 在索引层过滤
  - `where` = 可能还有别的条件需要回表判断（比如 SELECT * 返回了主键以外列）。

------

## 📌 4️⃣ 怎么看执行计划好不好？

一个执行计划是否健康，常看这几点：
 ✅ `type` 不要是 `ALL`（ALL = 全表扫描）
 ✅ `key` 是你期望的索引
 ✅ `rows` 越小越好（意味着过滤效果好）
 ✅ `Extra` 没有 `Using filesort`（表示无需额外排序）
 ✅ `Extra` 没有 `Using temporary`（表示没有创建临时表）
 ✅ 有 `Using index` 说明覆盖索引命中
 ✅ 有 `Using index condition` 说明 ICP 生效

------

## 📌 5️⃣ 怎么调试 ICP 和索引覆盖？

可以对比几种写法：

| SQL                                                | Extra                              | 是否 ICP | 是否覆盖索引 |
| -------------------------------------------------- | ---------------------------------- | -------- | ------------ |
| SELECT * WHERE age = 20                            | Using where                        | ❌        | ❌            |
| SELECT * WHERE age = 20 AND gender = 'M'           | Using index condition              | ✅        | ❌            |
| SELECT age, gender WHERE age = 20 AND gender = 'M' | Using index condition; Using index | ✅        | ✅            |

------

## 🔑 总结一句话

👉 **EXPLAIN = 拆解 SQL 执行过程的放大镜**
 👉 **看 ICP = 看 `Extra` 有没有 `Using index condition`**
 👉 **看覆盖索引 = 看 `Extra` 有没有 `Using index`**
 👉 **想让索引用得值回票价 = 条件写法、SELECT 列要“配合”索引列**

------

# 联合索引 + ICP + 覆盖索引示例

------

## 🎯 **示例场景**

假设我们有这样一个表 `user`：

```sql
CREATE TABLE user (
  id INT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(20),
  age INT,
  gender CHAR(1),
  city VARCHAR(20),
  KEY idx_age_gender_city (age, gender, city)
);
```

联合索引：`(age, gender, city)`

------

## 🌟 **准备样例数据**

```sql
INSERT INTO user (name, age, gender, city) VALUES
('Alice', 25, 'F', 'Beijing'),
('Bob', 30, 'M', 'Shanghai'),
('Charlie', 30, 'M', 'Beijing'),
('Diana', 40, 'F', 'Shenzhen'),
('Eric', 35, 'M', 'Guangzhou');
```

------

## 📌 **1️⃣ 只用到索引第 1 列**

```sql
EXPLAIN SELECT * FROM user WHERE age = 30;
```

**执行计划大概长这样：**

| id   | select_type | table | type | key                 | key_len | ref   | rows | Extra       |
| ---- | ----------- | ----- | ---- | ------------------- | ------- | ----- | ---- | ----------- |
| 1    | SIMPLE      | user  | ref  | idx_age_gender_city | 5       | const | 2    | Using where |

**解析：**

- `type` = ref（等值匹配）
- `key` = 用了联合索引
- `rows` = 2（age = 30 有 2 行）
- `Extra` = Using where（只用 age 做了索引匹配，剩下过滤靠回表）

------

## 📌 **2️⃣ 用到索引第 1 列 + 第 2 列，触发 ICP**

```sql
EXPLAIN SELECT * FROM user WHERE age = 30 AND gender = 'M';
```

**执行计划大概：**

| id   | select_type | table | type  | key                 | key_len | ref  | rows | Extra                 |
| ---- | ----------- | ----- | ----- | ------------------- | ------- | ---- | ---- | --------------------- |
| 1    | SIMPLE      | user  | range | idx_age_gender_city | 5       | NULL | 2    | Using index condition |

**解析：**

- `type` = range（age 是范围或等值）
- `key` = idx_age_gender_city
- `Extra` = Using index condition
  ➜ 说明 `gender = 'M'` 被下推到索引扫描阶段了。
- `Using where` 没有单独显示，因为已经在索引层过滤了 `gender`。

------

## 📌 **3️⃣ 用到索引第 1 列 + 第 2 列 + 第 3 列**

```sql
EXPLAIN SELECT * FROM user WHERE age = 30 AND gender = 'M' AND city = 'Beijing';
```

| id   | select_type | table | type | key                 | key_len | ref               | rows | Extra       |
| ---- | ----------- | ----- | ---- | ------------------- | ------- | ----------------- | ---- | ----------- |
| 1    | SIMPLE      | user  | ref  | idx_age_gender_city | 64      | const,const,const | 1    | Using where |

**解析：**

- `type` = ref（多个等值匹配）
- `key_len` = 所有列都命中
- `rows` = 1（只匹配一行）
- `Extra` = Using where（这里可能 ICP 没啥可下推的，因为都是等值，直接匹配了。）

------

## 📌 **4️⃣ 用覆盖索引，避免回表**

如果只查询索引包含的列：

```sql
EXPLAIN SELECT age, gender, city FROM user WHERE age = 30 AND gender = 'M' AND city = 'Beijing';
```

| id   | select_type | table | type | key                 | key_len | ref               | rows | Extra                    |
| ---- | ----------- | ----- | ---- | ------------------- | ------- | ----------------- | ---- | ------------------------ |
| 1    | SIMPLE      | user  | ref  | idx_age_gender_city | 64      | const,const,const | 1    | Using where; Using index |

**解析：**

- `Using index` 出现 ➜ 说明是**覆盖索引**（只从索引树读数据，不用回表）。
- 查询效率更高，磁盘 I/O 更少。

------

## 📌 **5️⃣ 如果写个范围查询，会截断后面的索引列**

```sql
EXPLAIN SELECT * FROM user WHERE age > 20 AND gender = 'M' AND city = 'Beijing';
```

| id   | select_type | table | type  | key                 | key_len | ref  | rows | Extra                 |
| ---- | ----------- | ----- | ----- | ------------------- | ------- | ---- | ---- | --------------------- |
| 1    | SIMPLE      | user  | range | idx_age_gender_city | 5       | NULL | 5    | Using index condition |

**解析：**

- `age > 20` 是范围条件 ➜ 后面的 `gender = 'M'` 还能下推（ICP）。
- `city` 就不能再用作索引匹配了 ➜ 因为范围条件会截断联合索引的连续匹配。

------

## 📌 **6️⃣ 联合索引在 B+ 树中的结构**

可以想象存储顺序像这样：

| age  | gender | city      | 主键 id |
| ---- | ------ | --------- | ------- |
| 25   | F      | Beijing   | 1       |
| 30   | M      | Beijing   | 3       |
| 30   | M      | Shanghai  | 2       |
| 35   | M      | Guangzhou | 5       |
| 40   | F      | Shenzhen  | 4       |

联合索引的本质就是一个按 `(age, gender, city)` 排序的有序集合。
 一旦跳过前缀，就没法高效定位范围了。

------

## ✅ **这张表你该怎么记？**

| 点           | 解释                                 |
| ------------ | ------------------------------------ |
| ICP          | `Extra` 出现 `Using index condition` |
| 覆盖索引     | `Extra` 出现 `Using index`           |
| 是否命中索引 | `key` 是否是预期索引                 |
| 是否回表     | 没有 `Using index` 就需要回表        |
| 最左前缀原则 | 只能连续用索引列                     |
| 范围会截断   | 出现范围条件，后面的列失效           |

------

## 🗂️ **小结**

✅ **EXPLAIN = 调试索引的放大镜**
 ✅ **ICP = 提前过滤，省回表**
 ✅ **覆盖索引 = 直接用索引数据页返回，不走主键页**
 ✅ **最左前缀原则 = 别跳列！**
 ✅ **范围条件 = 截断后续列匹配**

---

# 锁



mysql官方文档对锁的详细解释。

[MySQL :: MySQL 8.4 Reference Manual :: 17.7.1 InnoDB Locking](https://dev.mysql.com/doc/refman/8.4/en/innodb-locking.html)



一些文档

[MySQL锁、加锁机制（超详细）—— 锁分类、全局锁、共享锁、排他锁；表锁、元数据锁、意向锁；行锁、间隙锁、临键锁；乐观锁、悲观锁-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2431018)



## 锁到底是什么

MySQL中的锁是一个用于控制多个用户或进程访问同一数据库资源的机制。锁的主要目的是确保数据的完整性和一致性，特别是在多个用户或事务同时操作数据库时。MySQL锁的关键用途和目的包括：

#### 1. 保证数据一致性

锁机制确保在任何给定时间，只有允许的操作能够访问或修改数据。这帮助防止数据在未完成写操作的情况下被读取（称为“脏读”），或者未提交的数据被其他事务读到（称为“不可重复读”和“幻读”）。

#### 2. 防止更新冲突

当多个事务试图同时更新同一数据时，锁可以序列化访问，确保每个事务的更改不会与其他事务冲突，从而防止数据损坏和保证操作的原子性。

#### 3. 提高并发性能

通过合理利用不同类型和粒度的锁，MySQL可以在保障数据一致性和完整性的同时，优化系统的并发性能。例如，通过行级锁而非表级锁，可以减少资源竞争，使更多的用户或进程能够同时高效地工作。

#### 4. 支持不同的事务隔离级别

MySQL根据标准SQL定义提供了不同的事务隔离级别（读未提交、读提交、可重复读和串行化）。这些隔离级别通过使用不同的锁策略（如读锁、写锁、间隙锁等）来实现，以此来平衡并发性和数据正确性之间的需求。

#### 5. 事务管理

在事务处理中，锁是管理和保证事务ACID属性（原子性、一致性、隔离性、持久性）的关键工具。锁有助于确保事务要么完全执行，要么完全不执行，同时确保事务的结果即使在系统出错的情况下也能持久保存。

总结来说，MySQL中的锁是为了管理数据库中的并发操作，确保数据的一致性和完整性，同时也支撑事务的安全和有效执行。锁的设计和实现对数据库系统的性能和可靠性有直接影响。

## 以不同的维度了解锁

InnoDB是一个支持事务的存储引擎，提供高并发和事务完整性。了解不同维度的锁类型有助于更好地设计和优化数据库应用。以下是从几个不同的维度概述InnoDB的锁类型：

#### 1. 锁的粒度

- **行级锁**：最细的锁粒度，只锁定被直接访问的数据行。优势在于并发性高，但管理这些锁的成本也较高。
- **表级锁**：粗粒度锁，当操作涉及大量数据或全表操作时，可能会锁定整个表。表级锁执行速度快，但并发性较差。
- **间隙锁（Gap Locks）**：锁定一个范围内的空间，不仅涉及具体的行，还包括行之间的“间隙”。这种锁主要用于防止幻读。

#### 2. 锁的模式

- **共享锁（S Lock）**：允许一个事务读取一行，并防止其他事务获得相同数据行的排他锁。
- **排他锁（X Lock）**：允许事务更新或删除一行，并防止其他事务读取或修改同一行。
- **意向锁**：属于表级锁，分为意向共享锁（IS Lock）和意向排他锁（IX Lock），声明事务将要在表的行上请求共享锁或排他锁。

#### 3. 锁的目的

- **记录锁**：直接锁定目标记录。
- **间隙锁**：锁定一个范围内的记录，但不包括记录本身。用于保证读取的一致性和防止幻读。
- **临键锁（Next-Key Locks）**：是记录锁和间隙锁的组合，锁定一条记录及其前面的间隙，常用于范围查询的场景。

#### 4. 锁的功能

- **防止更新冲突**：通过排他锁确保一行数据在修改期间不被其他事务修改。
- **保持数据一致性**：使用共享锁和排他锁确保在事务处理过程中数据的读取和修改不会受到干扰。
- **防止幻读**：间隙锁和临键锁帮助实现可重复读的隔离级别，防止事务期间新的记录插入到结果集中。

通过这些不同维度的锁类型，InnoDB能够提供灵活且强大的数据处理能力，适用于要求高并发和高事务完整性的应用场景。理解并正确使用这些锁对于优化数据库性能和事务处理至关重要。

## 行锁的三种算法

MySQL的InnoDB存储引擎支持三种行锁算法：

1. **Record Lock**：单个行记录上的锁。这种锁会锁住具体的索引项。当SQL执行按照唯一性索引进行数据的检索时，查询条件等值匹配且查询的数据是存在，这时SQL语句加上的锁即为记录锁Record locks，锁住具体的索引项。
   
2. **Gap Lock**：间隙锁，锁定一个范围，但不包含记录本身。这种锁主要用于防止幻读。

3. **Next-Key Lock**：Record Lock + Gap Lock，锁定一个范围，并且锁定记录本身。Next-Key Lock 是结合了 Gap Lock 和 Record Lock 的一种锁定算法，在 Next-Key Lock 算法下，InnoDB 对于行的查询都是采用这种锁定算法。

这三种锁算法的选择取决于具体的操作和索引类型。例如，当我们查询的索引含有唯一属性时，Next-Key Lock会进行优化，降级为Record Lock，也就是说锁住的仅仅是索引本身，而不是范围。这样可以提高并发能力，但是可能会导致幻读问题。因此，具体使用哪种锁算法，需要根据实际的业务需求和系统性能来决定。

### 例子

[MySQL的锁机制-记录锁、间隙锁、临键锁](MySQL的锁机制-记录锁、间隙锁、临键锁-知乎.mhtml) 这篇文章写得不错



# 快照读和当前读

快照读（Snapshot Read）和当前读（Current Read）是数据库中常用的两种读取数据的方式，这两者主要探讨读取的数据版本问题。

1. **快照读（Snapshot Read）**：
   - 快照读是在事务开始时创建一个数据快照，并在整个事务过程中使用该快照来读取数据。换句话说，在事务执行期间，快照读始终读取事务开始时数据库中的数据状态。
   - 快照读可以保证事务在读取数据时不会受到其他事务的影响，即使其他事务在事务执行期间对数据进行了修改，也不会影响到快照读取的数据。

2. **当前读（Current Read）**：
   - 当前读是在执行读取操作时直接读取数据库中当前的数据状态。换句话说，当前读不会创建数据快照，而是直接读取数据库中最新的数据。
   - 当前读对于一些需要读取最新数据的场景非常有用，例如需要读取实时更新的数据或者执行短期操作的事务。

在 MySQL 中，根据使用的 SQL 语句不同，可以选择使用快照读或当前读：

- 当执行 `SELECT` 查询语句时，默认情况下使用的是快照读，即事务开始时读取的是快照而不受其他事务影响。如果需要读取最新的数据，可以使用 `FOR UPDATE` 或 `LOCK IN SHARE MODE` 来使用当前读。
- 当执行 `INSERT`、`UPDATE`、`DELETE` 等写入操作时，使用的是当前读，即直接操作数据库中当前的数据状态。

选择快照读还是当前读取决策取决于应用程序的需求和隔离级别的要求。快照读可以提供更高的并发性和隔离性，但当前读能够提供更接近实时的数据状态。

假设有两个事务，事务A和事务B，它们同时访问一个表中的某一行数据。我们假设初始时，数据如下：

```
| ID | Name  | Balance |
|----|-------|---------|
| 1  | Alice | 100     |
```

现在，事务A和事务B同时开始：

1. **事务A**：
   - 事务A读取当前数据，创建了自己的 Read-View。
   - 事务A执行一条更新语句，将余额减少到50。

2. **事务B**：
   - 事务B也读取当前数据，创建了自己的Read-View。
   - 事务B执行一条更新语句，将余额增加到150。

现在我们来分析具体过程：

- 当事务A读取数据时，它会创建自己的 Read-View，用于确定事务开始时数据库中的数据状态。因此，事务A读取到的余额是100，这是事务开始时的数据状态。这是一个快照读。
- 当事务A执行更新操作时，它会使用当前读，因为事务A需要确保更新的数据是基于最新的数据状态的。它执行的更新操作将余额减少到50。
- 由于事务A正在进行更新操作，所以事务B在执行更新操作之前必须等待。在这段等待期间，事务B仍然使用它开始时创建的 Read-View 来读取数据。它读取到的余额也是100，这也是一个快照读。
- 当事务B执行更新操作时，它也使用当前读。它执行的更新操作将余额增加到150。

综上所述，事务A和事务B在并发修改数据时，每个事务都会创建自己的 Read-View 来读取数据，以确保事务开始时读取的是一致的数据快照。而在执行更新操作时，它们都会使用当前读，以确保更新的数据是基于最新的数据状态。

# 一致性锁定读和一致性非锁定读

一致性非锁定读（Consistent Non-locking Reads）和一致性锁定读（Consistent Locking Reads）也是两种读取数据的方式，与快照读和当前读相比，主要探讨读操作是否锁定被读取的数据。

1. **一致性非锁定读**(快照读)：

   - 在一致性非锁定读中，数据库系统不会为读取操作设置任何锁，因此也称为非锁定读（Non-locking Read）。

   - 在这种读取方式下，事务可以读取数据而不会阻塞其他事务的读取或写入操作。

   - 一致性非锁定读通常用于读取事务中不需要严格一致性的数据，或者对读取的数据的即时性要求不高的情况。


2. **一致性锁定读**：

   - 一致性锁定读是通过设置适当的锁来确保读取的数据在整个事务过程中保持一致性的读取方式。

   - 通常会使用共享锁（Shared Lock）来保证其他事务可以同时读取数据，但阻止其他事务对数据进行修改。

   - 这种读取方式确保了数据在读取过程中的一致性，因为其他事务无法修改被读取的数据，直到当前事务完成。

   - 一致性锁定读适用于对读取的数据要求严格一致性的场景，尤其是在并发环境下保证读取的数据不被其他事务修改的情况下。


**`SELECT ... FOR UPDATE`** - 对读取的记录加一个X锁，其他事务不能对已锁定的行加上任何锁，**不影响其他读操作**

**`SELECT ... FOR SHARE`（旧版 `LOCK IN SHARE MODE`）** 对锁定的记录加一个S锁，其他事务可以加S锁，不能加X锁

# 自增长与锁

1. InnoDB 中对每个含有自增长值的表都有一个自增长计数器，当对含有自增长的计数器的表进行插入操作的时候， 这个计数器会被初始化：
   - `select max(auto_inc_col) from t for update`
   - 插入操作会依据这个自增长的计数器值加 1 赋予自增长列，这个实现方式叫做 AUTO-INC Locking，这个锁会在事务内的每一个 `insert` 语句执行完之后释放，而不是在整个事务结束时释放
2. **innodb_autoinc_lock_mode** 参数
   - 0 - 对于所有的 insert 都采用 AUTO-INC Locking，会持有锁一直到语句结束
   - 1 - 对于 simple insert 采用 mutex 对计数器进行访问，拿到释放锁， 这个对于复制来说是安全的，可以一次性拿到确定数目的自增 ID
   - 2 - 对于所有类型的 insert 都采用 mutex 来，性能最好，但是不安全，对于同一个 insert 来说，得到的 auto_increment 可能不是连续的。这是因为如果基于语句复制，在数据库 A 执行的时候语句 1 先获得 id，语句 2 后获得 id，而在数据库 B执行的时候是语句 2 先获得 id，所以会导致数据不一致。

# 自增ID什么时候会出现不连续

MySQL 的自增ID（`AUTO_INCREMENT`）虽然设计上是递增的，但在某些情况下会出现 **不连续** 的现象。以下是常见的原因和场景分析：

------

## **1. 事务回滚（Rollback）导致ID空缺**

### **现象**

- 事务插入数据后，如果发生回滚，已分配的自增ID **不会被回收**，导致后续ID跳过这些值。

### **示例**

```
-- 事务1
START TRANSACTION;
INSERT INTO users (name) VALUES ('Alice');  -- 假设分配ID=1
ROLLBACK;  -- 事务回滚，ID=1被丢弃

-- 事务2
INSERT INTO users (name) VALUES ('Bob');    -- 分配ID=2（跳过1）
```

**结果**：表中 `users` 的ID序列为 `2, 3, ...`，`1` 被跳过。

### **原因**

- MySQL 的自增ID是 **预先分配** 的（事务开始时分配ID，但事务回滚后ID不会被复用）。
- 这是为了 **避免并发事务的ID冲突**（如果回滚后复用ID，可能导致其他事务误用已回滚的数据）。

------

## **2. 插入失败（如唯一键冲突）**

### **现象**

- 如果插入数据时因 **唯一键冲突**（如 `PRIMARY KEY` 或 `UNIQUE KEY` 重复）导致失败，已分配的自增ID **同样不会被回收**。

### **示例**

```
-- 假设id=1已存在
INSERT INTO users (id, name) VALUES (1, 'Alice');  -- 唯一键冲突，插入失败
INSERT INTO users (name) VALUES ('Bob');           -- 分配ID=2（跳过1）
```

**结果**：表中 `users` 的ID序列为 `2, 3, ...`，`1` 被跳过。

### **原因**

- MySQL 在插入前会先分配自增ID，如果插入失败（如唯一键冲突），ID不会被回退。

------

## **3. 批量插入（Bulk Insert）的优化分配**

### **现象**

- 使用 `INSERT INTO ... VALUES (...), (...), ...` 批量插入时，MySQL 可能会 **一次性分配多个自增ID**（ID 申请呈 2 的指数倍增长，插入的数据非 2 的倍数时会导致 ID 不连续自增），但实际插入的行数可能少于分配的数量（如部分数据因冲突或过滤被跳过）。

### **示例**

```
-- 假设当前自增ID=1
INSERT INTO users (name) VALUES ('Alice'), ('Bob'), ('Alice');  -- 第三条因唯一键冲突失败
```

**结果**：表中 `users` 的ID序列为 `1, 2, 4`（`3` 被跳过，因为批量分配了ID=3但插入失败）。

### **原因**

- MySQL 为了提高性能，会在批量插入前 **预分配一组连续的ID**，即使部分数据插入失败，已分配的ID也不会被回收。

------

## **4. 手动插入指定ID**

### **现象**

- 如果手动插入一条记录并显式指定ID（如 `INSERT INTO users (id, name) VALUES (100, 'Alice')`），后续的自增ID会从 **最大值+1** 开始。

### **示例**

```
-- 当前自增ID=1
INSERT INTO users (id, name) VALUES (100, 'Alice');  -- 手动插入ID=100
INSERT INTO users (name) VALUES ('Bob');             -- 自增ID从101开始
```

**结果**：表中 `users` 的ID序列为 `1, 100, 101, ...`，`2~99` 被跳过。

### **原因**

- 手动指定的ID会 **强制更新自增计数器**（`AUTO_INCREMENT` 值变为 `MAX(id)+1`）。

------

## **5. 表删除或TRUNCATE后自增ID重置**

### **现象**

- 如果表被 `TRUNCATE`（清空表并重置自增计数器），或删除所有数据后 **某些存储引擎的行为差异**，可能导致自增ID重新开始。

### **示例**

```
TRUNCATE TABLE users;  -- 自增ID重置为1
INSERT INTO users (name) VALUES ('Alice');  -- ID=1
```

**对比**：

- `DELETE FROM users`：自增ID不会重置（下一条插入的ID是 `MAX(id)+1`）。
- `TRUNCATE TABLE users`：自增ID重置为1（相当于新建表）。

### **原因**

- `TRUNCATE` 是DDL操作（直接重建表），而 `DELETE` 是DML操作（逐行删除）。

------

## **6. 复制或主从同步的延迟**

### **现象**

- 在主从复制架构中，如果从库延迟或同步异常，可能导致从库的自增ID与主库不一致（但通常不会影响主库的连续性）。

### **原因**

- 主库的自增ID是严格递增的，但从库可能因复制延迟导致ID“看起来”不连续（实际主库仍是连续的）。

------

## **7. 其他特殊情况**

### **(1) 批量导入数据（如 `LOAD DATA INFILE`）**

- 批量导入时可能预分配大量ID，但部分数据因格式错误被跳过，导致ID空缺。

### **(2) 服务器重启（InnoDB引擎）**

- InnoDB 的自增计数器在内存中维护，但 **服务器异常重启后**，MySQL 会通过扫描表中的最大ID重新计算自增值（可能导致短暂的不连续，但通常很快恢复）。

------

## **总结：自增ID不连续的常见原因**

| **场景**               | **是否导致ID不连续** | **原因**                                           |
| ---------------------- | -------------------- | -------------------------------------------------- |
| 事务回滚               | ✅ 是                 | 已分配的ID不会被回收                               |
| 插入失败（唯一键冲突） | ✅ 是                 | 已分配的ID不会被回退                               |
| 批量插入部分失败       | ✅ 是                 | 预分配的ID可能多于实际插入的行数                   |
| 手动指定ID             | ✅ 是                 | 强制更新自增计数器                                 |
| `TRUNCATE` 表          | ✅ 是                 | 自增计数器重置为1                                  |
| `DELETE` 表数据        | ❌ 否                 | 自增ID继续递增（不会重置）                         |
| 主从复制延迟           | ⚠️ 看似不连续         | 从库同步延迟可能导致ID“跳跃”（主库实际仍是连续的） |

------

## **如何避免或处理自增ID不连续？**

1. **如果需要严格连续ID**：
   - 改用 **自定义序列**（如通过程序生成ID，或使用 `SEQUENCE` 对象（MySQL 8.0+ 支持））。
   - 使用 **业务层逻辑**（如订单号 `ORDER_20250211_001` 格式）替代自增ID。
2. **如果接受不连续ID**：
   - 自增ID的设计初衷是 **唯一性** 而非 **连续性**，大多数业务场景（如用户表、订单表）无需连续ID。
3. **监控和优化**：
   - 避免频繁事务回滚或插入失败。
   - 批量插入时预估数据量，减少ID浪费。

------

### **关键结论**

- **自增ID的不连续是正常现象**，通常由事务回滚、插入失败或批量操作导致。
- **自增ID的核心作用是保证唯一性**，而非连续性。如果业务依赖连续ID，需改用其他方案。

---

### **MySQL 自增 ID 用完了怎么办？**

MySQL 的 `AUTO_INCREMENT` 自增 ID 是有上限的，当达到最大值后，再插入数据时会报错。以下是详细分析和解决方案：

------

## **1. 自增 ID 的上限取决于数据类型**

不同的整数类型，自增 ID 的最大值不同：

| **数据类型** | **存储范围**                                                 | **最大值（MAX_VALUE）**                    | **是否可能用完**               |
| ------------ | ------------------------------------------------------------ | ------------------------------------------ | ------------------------------ |
| `TINYINT`    | -128 ~ 127（有符号）  0 ~ 255（无符号）                      | **255（UNSIGNED）**                        | ⚠️ 容易用完（小表也可能耗尽）   |
| `SMALLINT`   | -32,768 ~ 32,767（有符号）  0 ~ 65,535（无符号）             | **65,535（UNSIGNED）**                     | ⚠️ 小表可能耗尽                 |
| `MEDIUMINT`  | -8,388,608 ~ 8,388,607（有符号）  0 ~ 16,777,215（无符号）   | **16,777,215（UNSIGNED）**                 | ⚠️ 中等表可能耗尽               |
| `INT`        | -2,147,483,648 ~ 2,147,483,647（有符号）  0 ~ 4,294,967,295（无符号） | **4,294,967,295（UNSIGNED）**              | ❌ 大表也可能耗尽（但概率较低） |
| `BIGINT`     | -9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807（有符号）  0 ~ 18,446,744,073,709,551,615（无符号） | **18,446,744,073,709,551,615（UNSIGNED）** | 🚫 几乎不可能用完               |

**关键结论**：

- 如果使用 `TINYINT` 或 `SMALLINT`，表数据量稍大就会耗尽 ID。
- **生产环境强烈建议使用 `INT UNSIGNED` 或 `BIGINT UNSIGNED`**，避免 ID 用完的问题。

------

## **2. 自增 ID 用完后的表现**

当自增 ID 达到最大值后，再执行 `INSERT` 会报错：

```
ERROR 1467 (HY000): Failed to read auto-increment value from storage engine
```

或

```
ERROR 167 (22003): Out of range value for column 'id' at row 1
```

------

## **3. 解决方案**

### **(1) 修改字段类型为更大的数据类型（推荐）**

如果 ID 快要用完，可以修改表结构，将自增字段的数据类型改为更大的类型（如从 `INT` 改为 `BIGINT`）：

```
-- 1. 查看当前表结构
SHOW CREATE TABLE users;

-- 2. 修改字段类型（例如从 INT 改为 BIGINT UNSIGNED）
ALTER TABLE users MODIFY COLUMN id BIGINT UNSIGNED AUTO_INCREMENT;

-- 3. 重新插入数据（ID 会继续从最大值+1 开始）
INSERT INTO users (name) VALUES ('New User');
```

**优点**：

- 一劳永逸，几乎不会再遇到 ID 耗尽问题（`BIGINT UNSIGNED` 的最大值是 `18,446,744,073,709,551,615`）。
- 对现有数据无影响（仅扩展 ID 范围）。

**缺点**：

- 如果表数据量极大（如数十亿行），修改字段类型可能耗时较长（需锁表）。

------

### **(2) 重置自增 ID（谨慎使用）**

如果表数据可以删除或归档，可以重置自增 ID 的起始值：

```
-- 1. 查看当前最大 ID
SELECT MAX(id) FROM users;

-- 2. 重置自增 ID（例如从 1 开始）
ALTER TABLE users AUTO_INCREMENT = 1;
```

**适用场景**：

- 表数据可以清空或归档（如日志表、临时表）。
- **不适用于生产环境的核心业务表**（会导致 ID 重复，破坏唯一性）。

**风险**：

- 如果表中仍有数据，重置自增 ID 会导致后续插入的 ID 与已有数据冲突（报错）。

------

### **(3) 使用业务自定义 ID（替代方案）**

如果自增 ID 不是业务必需的，可以用其他方案替代：

- 

  UUID

  ：全局唯一，但存储空间大（32 字符），无序。

  ```
  INSERT INTO users (id, name) VALUES (UUID(), 'Alice');
  ```

- **雪花算法（Snowflake）**：分布式 ID 生成方案（如 Twitter 的 64 位 ID）。

- **业务自定义 ID**：如订单号 `ORDER_20250211_001` 格式。

**优点**：

- 不受自增 ID 限制。
- 分布式系统友好（如 UUID、雪花算法）。

**缺点**：

- 可能失去自增 ID 的简单性和查询效率（如 `WHERE id > 100` 的范围查询）。

------

### **(4) 分表或分库（超大数据量场景）**

如果单表数据量极大（如数十亿行），可以考虑：

- **水平分表**：按 ID 范围或哈希值拆分到多个表（如 `users_1`、`users_2`）。
- **分库分表**：使用中间件（如 ShardingSphere、MyCat）分散数据。

**优点**：

- 彻底解决单表 ID 耗尽问题。
- 提升查询和写入性能。

**缺点**：

- 架构复杂度高，需额外维护分片逻辑。

------

## **4. 如何预防自增 ID 耗尽？**

1. **设计阶段选择合适的数据类型**：

   - 生产环境默认使用 `INT UNSIGNED`（最大 42 亿）或 `BIGINT UNSIGNED`（最大 1844 亿亿）。
   - 避免使用 `TINYINT` 或 `SMALLINT`。

2. **监控 ID 使用情况**：

   - 定期检查当前最大 ID 和剩余可用 ID：

     ```
     SELECT MAX(id) FROM users;
     -- 计算剩余可用 ID（例如 INT UNSIGNED 最大 4,294,967,295）
     SELECT 4294967295 - (SELECT MAX(id) FROM users) AS remaining_ids;
     ```

3. **提前规划扩容**：

   - 如果表数据增长快，提前从 `INT` 升级到 `BIGINT`。

------

## **5. 总结**

| **问题**     | **解决方案**                      | **适用场景**   |
| ------------ | --------------------------------- | -------------- |
| 自增 ID 用完 | 修改字段类型为 `BIGINT UNSIGNED`  | 生产环境核心表 |
| 自增 ID 用完 | 重置自增 ID（仅限可清空数据的表） | 日志表、临时表 |
| 自增 ID 用完 | 改用 UUID 或雪花算法              | 分布式系统     |
| 自增 ID 用完 | 分表或分库                        | 超大数据量场景 |

**核心建议**：

- **生产环境务必使用 `BIGINT UNSIGNED`**，避免 ID 耗尽问题。
- 如果 ID 已耗尽，优先通过 `ALTER TABLE` 扩展数据类型，而非重置 ID。

# InnoDB 的 MVCC 机制

1. **版本号**：
   - 每个事务在执行修改操作时都会分配一个唯一的事务ID（Transaction ID），用于标识该事务的提交顺序。这些事务ID被用来构建数据的不同版本。

2. **版本链**：
   - 对于每一行数据，在每次修改时都会创建一个新版本，并将该版本与对应的事务ID关联起来。这些版本被组织成一个版本链，形成了数据的不同历史版本。

3. **Read-View**：
   - 当事务开始时，会创建一个 Read-View，用于确定事务开始时数据库中数据的状态。Read-View 包含了当前事务的事务ID以及其他已提交事务的最大事务ID。
   - 在执行读取操作时，事务只能看到比自己的事务ID小或等于自己事务ID的数据版本，这样可以确保事务在读取数据时不会看到未提交的修改，从而实现了事务的隔离性。
   - 快照读sql 执行时 mvcc提取数据的依据, 就是在执行查询 sql的时候一个事务表, 通过事务表和数据保存的事务信息, 来判断当前的 sql应该获取哪个时刻的数据才是安全的
     m_ids - 当前活跃的事务编号, 还没有被提交的事务
     min_trx_id - 最小活跃事务编号
     max_trx_id - 预分配事务编号, 当前最大的事务编号 + 1
     creator_trx_id - ReadView创建者的事务编号
   - 可重复读和读提交 - 基于 undo_log 的版本链, 在表内会额外的增加两个字段 trx_id(这数据属于哪个事务编号, 修改操作的事务编号), db_roll_ptr(指向上一个进行版本变化时的数据镜像)
     1. 判断当前事务编号是否等于 creator_trx_id, 成立说明是当前事务更改的, 直接返回 - 也就是说, 第一步判断当前数据库的数据是不是当前事务所修改的, 自己修改的数据自己肯定可以访问的
     2. 判断 trx_id < min_trx_id, 成立说明数据已经提交, 可以返回 - 这个说明我们的 ReadView 最小的事务都要比这个数据的事务小, 对于当前的我们来说, 这个数据百分百安全
     3. 判断 trx_id > max_trx_id, 成立则说明事务是在 ReadView 生成之后才开启的, 不允许访问数据 - 当前数据库里的数据, 是在我们的 ReadView 生成之后才被提交过,
        我们不能在过去查看未来的数据, 所以是不可以用这个数据的
     4. 判断 min_trx_id <= trx_id <= max_trx_id, 成立则在 m_ids 对比, 如果不存在数据则代表是已提交的, 可以访问 - 这个数据如果是在最小事务和最大事务之间修改的,
        就是说这个数据是在这个 ReadView 被创建的期间被改的,
        如果不在活跃事务里, 就说明数据对于当前的 ReadView 来说安全, 当然可以访问
        读提交(RC)每次都会生成 ReadView 来获取数据, 也就意味着, 在同一个事务内, 会出现不可重复读
        可重复读(RR)仅在第一次会生成 ReadView, 之后都会复用这个 ReadView, 所以这意味着避免了不可重复读和幻读
        但是有特例, 在同一个事务内, 两次快照读中间穿插一次当前读会导致幻读, 这种情况, 会在第二次快照读的时候重新创建 ReadView
        案例: 在第一次查询的时候创建 ReadView, 然后事务二新插入了一条数据并提交成功, 事务一更新全表数据的某个字段, 然后执行查询操作这时就产生了幻读

4. **Read-View 的隔离级别**：
   - 不同的隔离级别会影响 Read-View 的创建方式和数据可见性。例如，在 Read Committed 隔离级别下，Read-View 只会包含当前事务开始时已提交的事务的最大事务ID；而在 Repeatable Read
     隔离级别下，Read-View
     会在事务执行期间保持不变，即使其他事务提交也不会改变它。

可以想象有一张表，其中包含一行数据，例如：

```
| ID | Name   | Age |
|----|--------|-----|
| 1  | Alice  | 30  |
```

假设有两个事务 T1 和 T2，它们分别对相同的行进行了修改。每次对行的修改都会产生一个新版本。例如，T1 和 T2 分别对 Age 字段进行了修改，形成了如下的版本链：

```
Initial State: 
| ID | Name   | Age |
|----|--------|-----|
| 1  | Alice  | 30  |

After T1:
| ID | Name   | Age |
|----|--------|-----|
| 1  | Alice  | 35  |  (T1 修改 Age)

After T2:
| ID | Name   | Age |
|----|--------|-----|
| 1  | Alice  | 40  |  (T2 修改 Age)
```

在这个示例中，每个版本都与对应的事务关联起来。当一个事务开始时，它会创建一个 Read-View，该 Read-View 包含了事务开始时数据库中已提交的数据的版本。根据隔离级别的不同，Read-View
可能会包含更旧的或更新的版本。例如，Repeatable Read 隔离级别下，T1 和 T2
在执行期间始终看到它们开始时的版本，即 T1 看到 Age 是 30，而 T2 看到 Age 是 35。

通过版本链和 Read-View，InnoDB 实现了 MVCC 机制，从而能够在高并发环境下提供高性能的事务支持。

# 数据库乐观锁和悲观锁的实现

要在Java中实现对MySQL数据库的乐观锁和悲观锁，可以使用以下策略和代码示例：

### 乐观锁的实现

乐观锁通常通过在数据库表中增加一个 **版本号字段（如 `version`）** 来实现。每次更新记录时，都会检查版本号，并在更新时将其递增。

假设你有一个名为 `products` 的表，其中包含 `id`、`name`、`price` 和 `version` 字段。

1. **数据库表结构**：

   ```sql
   CREATE TABLE products (
     id INT AUTO_INCREMENT PRIMARY KEY,
     name VARCHAR(255),
     price DECIMAL(10, 2),
     version INT DEFAULT 0
   );
   ```

2. **Java中的乐观锁实现**：

   使用JDBC或JPA来执行操作。这里以JDBC为例：

   ```java
   import java.sql.*;
   
   public class OptimisticLockTest {
   
       public void updateProduct(int productId, String newName, double newPrice, int oldVersion) {
           String query = "UPDATE products SET name = ?, price = ?, version = version + 1 WHERE id = ? AND version = ?";
           try (Connection conn = DriverManager.getConnection("your-database-url", "username", "password");
                PreparedStatement stmt = conn.prepareStatement(query)) {
               
               conn.setAutoCommit(false); // 开启事务
   
               stmt.setString(1, newName);
               stmt.setDouble(2, newPrice);
               stmt.setInt(3, productId);
               stmt.setInt(4, oldVersion);
   
               int affectedRows = stmt.executeUpdate();
   
               if (affectedRows == 0) {
                   throw new IllegalStateException("Update failed: the product was modified by another transaction.");
               }
   
               conn.commit(); // 提交事务
           } catch (SQLException e) {
               e.printStackTrace();
           }
       }
   }
   ```

### 悲观锁的实现

悲观锁通常使用数据库本身的锁机制来实现，如在SELECT语句中使用 `FOR UPDATE` 以在事务内锁定行。

假设使用相同的 `products` 表。

1. **Java中的悲观锁实现**：

   ```java
   import java.sql.*;
   
   public class PessimisticLockTest {
   
       public void updateProductWithLock(int productId, String newName, double newPrice) {
           String selectQuery = "SELECT * FROM products WHERE id = ? FOR UPDATE";
           String updateQuery = "UPDATE products SET name = ?, price = ? WHERE id = ?";
           try (Connection conn = DriverManager.getConnection("your-database-url", "username", "password");
                PreparedStatement selectStmt = conn.prepareStatement(selectQuery);
                PreparedStatement updateStmt = conn.prepareStatement(updateQuery)) {
               
               conn.setAutoCommit(false); // 开启事务
   
               selectStmt.setInt(1, productId);
               try (ResultSet rs = selectStmt.executeQuery()) {
                   if (rs.next()) {
                       updateStmt.setString(1, newName);
                       updateStmt.setDouble(2, newPrice);
                       updateStmt.setInt(3, productId);
                       updateStmt.executeUpdate();
                   }
               }
   
               conn.commit(); // 提交事务
           } catch (SQLException e) {
               e.printStackTrace();
           }
       }
   }
   ```

这些示例提供了使用Java对MySQL数据库进行乐观锁和悲观锁操作的基本方法。你可以根据具体的业务需求进行调整和优化。



# 关于 CHARSET 和 COLLATE

[再见乱码：5分钟读懂MySQL字符集设置 - 程序猿小卡 - 博客园](https://www.cnblogs.com/chyingp/p/mysql-character-set-collation.html)

[MYSQL中的COLLATE是什么？ - 腾讯云开发者 - 博客园](https://www.cnblogs.com/qcloud1001/p/10033364.html)



UTF-8（UTF-8MB3）与UTF-8MB4的基本区别

- **UTF-8（也称为UTF-8MB3）**：这是MySQL中的一种字符集，它支持最长为三个字节的字符。这意味着它可以存储基本多语言平面（BMP）中的字符，但不支持四字节的Unicode字符，如某些罕见的汉字、表情符号或新加入Unicode的字符。
- **UTF-8MB4**：这是UTF-8的超集，它支持最长为四个字节的字符。这使得它能够存储所有Unicode字符，包括BMP之外的字符和表情符号等。从MySQL 5.5.3版本开始支持UTF-8MB4。



# MySQL的默认库



MySQL默认的四个系统库及其作用如下：

---

### **1. `information_schema`**
- **作用**：存储数据库的**元数据**信息，提供对数据库结构的只读访问。
- **关键内容**：
  - 数据库、表、列、索引、权限等元数据（如`TABLES`、`COLUMNS`、`SCHEMATA`表）。
  - 视图、存储过程、触发器等对象信息。
- **特点**：
  - 数据通过**视图**动态生成，不直接存储数据。
  - 用户不可修改，用于查询数据库结构（如`SHOW`命令的底层实现）。

---

### **2. `mysql`**
- **作用**：存储**用户账户、权限、系统配置**及核心数据。
- **关键内容**：
  - 用户与权限表（如`user`、`db`、`tables_priv`）。
  - 存储过程、事件、触发器定义。
  - 时区配置、日志配置（如`time_zone`、`general_log`）。
- **特点**：
  - 直接修改可能导致系统不稳定，建议使用`GRANT`、`ALTER USER`等SQL语句管理。
  - 包含关键系统数据（如`root`用户密码）。

---

### **3. `performance_schema`**
- **作用**：收集MySQL服务器的**运行时性能数据**，用于监控与分析。
- **关键内容**：
  - 资源使用情况（CPU、内存、I/O）。
  - 查询执行细节、锁竞争、连接信息。
  - 事件监控（如`events_statements_summary_by_digest`）。
- **特点**：
  - 数据基于配置项动态采集，默认部分功能开启。
  - 用于深度性能调优，表结构复杂，需结合文档使用。

---

### **4. `sys`**
- **作用**：提供**简化视图与工具**，辅助性能分析与诊断。
- **关键内容**：
  - 基于`information_schema`和`performance_schema`的预定义视图（如`schema_table_statistics`）。
  - 常用性能指标汇总（如高负载查询、索引使用情况）。
- **特点**：
  - 专为DBA设计，简化复杂性能查询（如`sys.session`视图）。
  - 无独立数据，依赖底层系统库，MySQL 5.7+版本默认集成。

---

### **注意事项**
- **版本差异**：`sys`库在MySQL 5.7及以上版本默认存在，旧版本需手动安装。
- **安全操作**：避免直接修改系统库表（如`mysql.user`），优先使用专用SQL语句。
- **性能开销**：`performance_schema`可能占用资源，可按需配置监控项。

通过这四个库，MySQL实现了元数据管理、权限控制、性能监控与优化支持的核心功能。



---

# 事务

MySQL是一种流行的关系型数据库管理系统（RDBMS），它支持事务的概念，可以确保数据库操作的原子性、一致性、隔离性和持久性（ACID属性）。

以下是关于MySQL事务的简单介绍：

1. **事务的概念**：事务是一组数据库操作，这些操作要么全部执行成功，要么全部失败，没有中间状态。事务是数据库管理系统中实现数据一致性和完整性的重要机制。

2. **事务的ACID特性**：
   - **原子性（Atomicity）**：事务中的所有操作要么全部执行成功，要么全部失败，没有部分执行的情况。如果一个操作失败，则整个事务会被回滚到事务开始前的状态。
   - **一致性（Consistency）**：事务执行后，数据库从一个一致的状态转换到另一个一致的状态。即使在事务执行过程中发生错误，数据库也不会处于矛盾或不一致的状态。
   - **隔离性（Isolation）**：事务的执行不受其他事务的影响，即使多个事务同时执行，它们之间也相互独立。每个事务都应该感知到数据库在其执行期间保持一致性的状态。
   - **持久性（Durability）**：一旦事务提交，其结果应该永久保存在数据库中，即使发生系统崩溃或故障，数据也不会丢失。

3. **事务的控制语句**：MySQL提供了以下几个常用的事务控制语句：
   - `START TRANSACTION`、`begin`：开始一个新的事务。
   - `COMMIT`：提交当前事务，将所有的操作持久化到数据库中。
   - `ROLLBACK`：回滚当前事务，撤销所有的未提交操作，恢复到事务开始前的状态。

4. **事务隔离级别**：MySQL支持不同的事务隔离级别，包括读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。这些隔离级别决定了事务之间相互影响的程度。



# log

Undolog、Redolog和Binlog在MySQL中扮演着不同的角色，它们的出现分别解决了数据库的不同问题。

1. **Undo日志（Undolog）**：
   - Undo日志记录了事务执行前的数据状态，主要用于事务的回滚和MVCC（多版本并发控制）机制的实现。
   - 当事务执行更新操作时，Undo日志记录了更新前的数据状态，以便在事务回滚或者数据库崩溃恢复时将数据恢复到更新前的状态。

2. **Redo日志（Redolog）**：
   - Redo日志记录了事务执行后的数据修改操作，主要用于数据库的崩溃恢复和数据持久性的保证。
   - 当事务执行更新操作时，Redo日志记录了更新操作的具体细节，包括修改的页号、偏移量以及修改后的数据值，以便在数据库崩溃后重新应用修改操作。

3. **Binlog（Binary Log）**：
   - Binlog记录了数据库中所有的数据修改操作，主要用于数据备份、数据恢复、数据库复制和数据审计等场景。
   - Binlog记录了每个数据修改操作的逻辑细节，包括事务的提交、回滚以及DDL语句的执行，通过分析Binlog文件可以实现对数据库的数据备份、恢复、复制和审计。

总的来说，Undolog主要解决了事务的原子性和隔离性问题，Redolog主要解决了数据库的持久性问题，而Binlog主要用于数据备份、恢复、复制和审计等用途。这三种日志文件各自发挥着重要的作用，共同保证了数据库的可靠性、一致性和安全性。

---

## undo_log

MySQL的undo日志（Undo Log），记录事务修改前的数据，用于事务失败时的回滚。

主要用于实现事务的原子性和多版本并发控制（MVCC）

1. **作用**：
   - 记录事务执行过程中对数据的修改操作。
   - 用于事务的回滚操作，当事务发生错误或者被撤销时，MySQL可以利用Undo日志将数据恢复到事务开始前的状态。
   - 用于实现MVCC（多版本并发控制）机制，当事务需要读取历史版本的数据时，MySQL可以利用Undo日志提供相应的数据版本。
2. **记录内容**：
   - 对于INSERT：记录主键信息，回滚时执行DELETE
   - 对于DELETE：记录完整行数据，回滚时执行INSERT
   - 对于UPDATE：记录被修改前的数据，回滚时恢复原值
3. **Undo日志的存储方式**：
   - Undo日志通常存储在磁盘上的Undo表空间中，它是一个特殊的系统表空间。
   - MySQL通过共享表空间的方式管理Undo日志，多个数据库共享同一个Undo表空间。
4. **Undo日志的生命周期**：
   - Undo日志的生命周期与事务的生命周期密切相关。当事务开始时，MySQL会为该事务分配一块Undo日志区域，用于记录该事务执行过程中的修改操作。
   - 当事务提交或者回滚时，相关的Undo日志区域会被释放，以便重用。
5. **性能优化**：
   - MySQL的InnoDB存储引擎支持Undo日志的优化技术，包括Undo表空间的自动扩展、Undo日志的延迟写入、Undo日志的压缩等，以提高Undo日志的性能和效率。

---

## redo_log

redo日志（Redo Log，重做日志），保证原子性和持久性。

1. **作用**：
   - 记录事务对数据库进行的所有修改操作，包括插入、更新和删除等操作。
   - 用于事务的持久化，确保在数据库发生故障或崩溃时可以将事务的修改操作重新应用到数据库中，从而保证数据的一致性和完整性。

2. **记录内容**：
   - Redo日志记录了事务对数据库进行的所有修改操作，但与Undo日志不同，它不记录修改前的数据内容，而是记录了修改操作的具体细节，如修改的页号、偏移量以及修改后的数据值。
   - Redo日志记录的信息是物理日志，它记录了数据页的物理变化。

3. **Redo日志的存储方式**：
   - Redo日志通常存储在磁盘上的Redo日志文件中，这些文件是MySQL的数据目录中的特殊文件。
   - MySQL会定期将Redo日志缓冲区中的日志写入Redo日志文件，以保证数据的持久化。

4. **Redo日志的生命周期**：
   - Redo日志的生命周期与事务的生命周期密切相关。当事务提交时，相关的Redo日志会被写入Redo日志文件，以确保事务的持久化。
   - Redo日志的写入操作是顺序写入，因此效率比较高。同时，Redo日志是循环使用的，当Redo日志文件达到一定大小或者发生满足条件的检查点时，会被重用。

5. **性能优化**：
   - MySQL的InnoDB存储引擎支持Redo日志的优化技术，包括Redo日志的组提交、Redo日志的延迟写入、Redo日志的缓冲等，以提高Redo日志的性能和效率。

---

## undo_log和redo_log结合

**Undolog和Redolog详细操作过程：**

1. **事务开始**：事务T1开始。

2. **生成Undo日志**：数据库为事务T1分配Undo日志区域，并在Undo日志中记录当前余额的修改前的值（1000元）。

3. **生成Redo日志**：数据库为事务T1分配Redo日志区域，并在Redo日志中记录当前余额的修改操作的具体细节，包括修改的页号、偏移量以及修改后的数据值。

4. **执行更新操作**：事务T1执行一条更新语句，将某个客户的余额从1000元增加到2000元。

5. **生成Undo日志**：在执行更新操作时，数据库再次在Undo日志中记录修改前的数据内容（1000元），以备将来回滚。

6. **生成Redo日志**：在执行更新操作时，数据库在Redo日志中记录将余额增加到2000元的具体修改操作的细节。

7. **提交事务**：事务T1提交。

8. **将Redo日志写入磁盘**：数据库将Redo日志中的数据刷新到数据库的磁盘文件中，确保数据的持久性。

**如何判断事务是否已经提交：**

在MySQL中，可以通过检查Undo日志和Redo日志的状态来判断事务是否已经提交：

- 如果Undo日志中记录了事务T1的修改前的数据内容，但是Redo日志中没有相应的修改操作，则表示事务T1尚未提交。
- 如果Undo日志和Redo日志中都记录了事务T1的修改操作，且Redo日志已经写入磁盘，则表示事务T1已经提交。

**数据库宕机后的操作：**

在数据库宕机后，根据Undo日志和Redo日志的状态，可以进行以下操作：

- 如果Undo日志中记录了事务T1的修改前的数据内容，但是Redo日志中没有相应的修改操作，则需要回滚未提交的事务T1。
- 如果Undo日志和Redo日志中都记录了事务T1的修改操作，且Redo日志已经写入磁盘，则不需要进行回滚，因为事务T1已经提交。

---

## binlog

MySQL的binlog（Binary Log）是MySQL数据库引擎提供的一种日志文件，用于记录数据库中所有的数据修改操作，包括插入、更新和删除等操作。与Redo日志类似，binlog也是一种用于实现事务的持久性和恢复性的重要组件。

以下是关于MySQL的binlog的一些重要信息：

1. **作用**：
   - 记录数据库中的所有数据修改操作，包括事务的提交、回滚以及DDL（Data Definition Language）语句的执行等。
   - 用于数据备份、数据恢复、数据库复制和数据审计等用途。

2. **记录内容**：
   - Binlog文件以二进制格式记录了每个数据修改操作的具体细节，包括修改操作的类型、受影响的数据行、修改前的数据内容以及修改后的数据内容等。
   - Binlog文件记录的信息是逻辑日志，它记录了数据修改操作的逻辑细节。

3. **存储位置**：
   - Binlog文件默认存储在MySQL的数据目录中，文件名通常以"mysql-bin"开头，后面跟着一串数字表示文件序号。例如，mysql-bin.000001、mysql-bin.000002等。
   - 可以通过配置文件或者动态参数来指定Binlog文件的存储路径和文件名格式。

4. **使用场景**：
   - 数据备份与恢复：通过恢复Binlog文件中的数据修改操作，可以实现对数据库的定点恢复或者将数据库恢复到某个特定时间点的状态。
   - 数据库复制与同步：通过将Binlog文件复制到其他MySQL实例上，可以实现数据库的主从复制、主从同步和数据分发等功能。
   - 数据审计与性能分析：通过分析Binlog文件中的数据修改操作，可以进行数据审计和性能分析，了解数据库的使用情况和性能瓶颈。

综上所述，MySQL的binlog是一种重要的日志文件，用于记录数据库中的所有数据修改操作，实现事务的持久性、恢复性和数据库的复制与同步等功能。通过合理管理和利用binlog，可以提高数据库的可靠性、可用性和安全性。

---

# MySQL为什么使用可重复读作为数据库的默认隔离级别

## 🎯 **先简单回顾四个隔离级别**

| 隔离级别         | 脏读 | 不可重复读 | 幻读                     |
| ---------------- | ---- | ---------- | ------------------------ |
| READ UNCOMMITTED | 可能 | 可能       | 可能                     |
| READ COMMITTED   | 避免 | 可能       | 可能                     |
| REPEATABLE READ  | 避免 | 避免       | 可能（但InnoDB可避免）   |
| SERIALIZABLE     | 避免 | 避免       | 避免（通过强制串行执行） |

------

## 🔍 **为什么是 REPEATABLE READ？**

### ✅ 1️⃣ **更强的可读一致性**

- 和 `READ COMMITTED` 相比，`REPEATABLE READ` 保证在同一个事务里多次读相同数据时结果一致。
- 对大多数业务场景来说，这就是用户通常期望的：“同一个事务里读到的东西不会变”。
- 避免了不可重复读问题（同一事务内多次读同一行发现值变了）。

------

### ✅ 2️⃣ **InnoDB 的多版本并发控制（MVCC）天然支持**

- InnoDB 用 MVCC 来实现 `REPEATABLE READ` 很高效。
- 它通过生成 *一致性视图（snapshot）* 来保证事务开始后读到的是事务开始时刻的数据快照。
- 不需要对读加锁，提升了并发性能。

------

### ✅ 3️⃣ **幻读也能通过 Next-Key Lock 避免**

- 在 SQL 标准里，`REPEATABLE READ` 仍然会有幻读（比如，范围查询后又插入了新行）。
- 但是 InnoDB 在 `REPEATABLE READ` 下对 *范围查询* 会加 **Next-Key Lock**（记录锁 + 闭区间间隙锁），可以避免幻读。
- 这使得 `REPEATABLE READ` 在 InnoDB 下，**效果接近 SERIALIZABLE，但又比 SERIALIZABLE 并发更高**。

------

### ✅ 4️⃣ **比 SERIALIZABLE 性能更好**

- `SERIALIZABLE` 通过锁表或锁范围，强制所有事务串行执行，性能很差，一般只在极端一致性要求下使用。
- `REPEATABLE READ` + Next-Key Lock 已经足够满足绝大多数一致性需求，同时还能保持高并发。

------

## 🔑 **结论**

MySQL/InnoDB 选择 `REPEATABLE READ` 作为默认隔离级别的核心原因：

- ⚖️ **一致性：** 满足多数 OLTP 场景对一致性的需求
- 🚀 **性能：** 通过 MVCC 和 Next-Key Lock，在保证一致性的同时最大化并发
- 🧩 **实现：** InnoDB 的存储引擎结构非常适合这种隔离级别

------

## 📌 **要点小结**

| 隔离级别        | InnoDB MVCC 支持    | 并发性能 | 是否防幻读               |
| --------------- | ------------------- | -------- | ------------------------ |
| READ COMMITTED  | 支持                | 高       | 否                       |
| REPEATABLE READ | 支持                | 高       | 是（通过 Next-Key Lock） |
| SERIALIZABLE    | 不仅 MVCC，还要加锁 | 低       | 是（串行化）             |

------

如果你需要，我可以再帮你画个隔离级别 + MVCC + 锁的交互示意图，要不要？🎨



# MySQL如何在可重复读解决幻读的

## 📌 1️⃣ 先明确：什么是幻读？

- **幻读**：同一个事务中，两次执行相同条件的查询，却出现了“新增”或“消失”的记录。

- 常见场景：

  - 事务 T1：

    ```sql
    START TRANSACTION;
    SELECT * FROM employees WHERE age > 30; -- 查询到10行
    ```

  - 另一个事务 T2：

    ```sql
    INSERT INTO employees (name, age) VALUES ('Alice', 35);
    COMMIT;
    ```

  - T1 里再次执行：

    ```sql
    SELECT * FROM employees WHERE age > 30; -- 现在是11行，多了一行
    ```

  - 这就是幻读：**范围内插入了“幻影”行。**

------

## 📌 2️⃣ MVCC 能防什么，防不了什么？

- **MVCC**（多版本并发控制）是 InnoDB 的核心机制。
  - 它通过“快照”读来避免不可重复读（同一行值被改了，看到的还是旧值）。
- 但是 **MVCC 只能防止行的修改/删除导致的可见性变化**，对 *新插入的行* 无法避免：
  - 因为新插入的行有新的版本号，新事务可见。
- 所以单靠 MVCC，幻读还是存在。

------

## 📌 3️⃣ InnoDB 是怎么真正避免幻读的？

### 🎯 **核心武器：Next-Key Lock（间隙锁 + 记录锁）**

#### 🔹 什么是 Next-Key Lock？

- InnoDB 解决幻读，依赖 **Next-Key Lock**，它由：
  - **记录锁（Record Lock）**：锁住已存在的行。
  - **间隙锁（Gap Lock）**：锁住两个已存在记录之间的“空隙”。

#### 📌 例子

假设表里有：

| id   |
| ---- |
| 10   |
| 20   |
| 30   |

执行：

```sql
SELECT * FROM t WHERE id > 10 AND id < 30 FOR UPDATE;
```

InnoDB 会：

- 锁住 id=20（记录锁）
- 锁住 (10, 20) 之间的间隙
- 锁住 (20, 30) 之间的间隙

所以：

- 其他事务不能在 (10, 30) 范围内插入新行
- 即便新行符合条件，也插不进来，从而防止了幻读

------

### 📌 4️⃣ 为什么普通 SELECT 不加锁？

- `SELECT * FROM t WHERE ...` **是快照读**（一致性读），只依赖 MVCC，不加锁。
- `SELECT * FROM t WHERE ... FOR UPDATE` 或 `LOCK IN SHARE MODE` 是 **当前读**，需要锁住行 + 间隙，才防止幻读。

------

## 📌 5️⃣ 在 REPEATABLE READ 下，什么时候会防幻读？

✅ **InnoDB 的 REPEATABLE READ + 当前读**（加锁查询）= 幻读可防
 ❌ **REPEATABLE READ + 快照读**（普通 SELECT）= 只能防止不可重复读，无法直接防幻读

所以：

- **DML 语句（UPDATE / DELETE）** 天生是当前读，会自动加 Next-Key Lock → 防止幻读。

- 如果只是普通 SELECT，要防幻读，必须显示加锁：

  ```sql
  SELECT * FROM t WHERE ... FOR UPDATE;
  ```

------

## 📌 6️⃣ 为什么不直接用 SERIALIZABLE？

- `SERIALIZABLE` 级别下，所有读都会隐式加锁，相当于所有范围读都变成 `FOR UPDATE`。
- 并发性能非常差，除非必要，实际很少用。
- InnoDB 的 `REPEATABLE READ` + Next-Key Lock 已经能在需要的地方避免幻读了，更灵活高效。

------

## ✅ **总结**

| 机制          | 防不可重复读 | 防幻读 | 是否锁间隙   |
| ------------- | ------------ | ------ | ------------ |
| MVCC 快照读   | ✅            | ❌      | 否           |
| Next-Key Lock | ✅            | ✅      | 是           |
| SERIALIZABLE  | ✅            | ✅      | 是（更严格） |

- **MVCC** → 保证快照一致性，防不可重复读。
- **Next-Key Lock** → 锁住已存在记录和间隙，防止新行插入，解决幻读。

------

**一句话记忆：**

> *InnoDB 用 MVCC + Next-Key Lock，在可重复读下把幻读关进笼子里！*

------



# 数据库碎片整理方案





# MySQL中一条语句的执行流程

好问题！
 下面我给你详细梳理一下：**在 MySQL 中，一条 SQL 语句从客户端发出去到执行完成的完整流程**（以 `SELECT` 为例，其他 DML/DDL 也是类似的大框架）。

------

## 🚦 **1️⃣ 客户端发送 SQL**

- 客户端通过 **连接器**（Connector）把 SQL 发送到 MySQL 服务器。
- 连接器负责：
  - 验证连接是否有效
  - 进行认证（用户名、密码、权限）
  - 管理连接池（比如长连接、短连接）

------

## 🔒 **2️⃣ 连接层验证权限**

- 进入 MySQL 后，先通过 `user` 表检查当前用户是否有权限执行这条语句。
- 如果没有权限，直接返回错误。

------

## 📚 **3️⃣ 查询缓存（MySQL 5.7及以前）**

- 老版本 MySQL 有查询缓存（`query_cache`），如果 SQL 一模一样且表没有被更新，结果可能直接从缓存返回。
- **InnoDB 不太依赖查询缓存**，8.0 已移除。

------

## 🧩 **4️⃣ SQL 解析器（Parser）**

- 如果没有命中缓存，SQL 会先进入 **解析器**（Parser）。
- 功能：
  - 词法分析：把 SQL 拆成 token（关键字、表名、列名等）
  - 语法分析：根据语法规则（BNF），生成 **解析树（Parse Tree）**

------

## 🗂️ **5️⃣ 预处理器（Preprocessor）**

- 对解析树做语义分析：
  - 检查表名、列名是否存在
  - 检查字段是否有访问权限
  - 解析 `*`，转换成具体列名
- 得到一个逻辑执行计划的雏形。

------

## 📈 **6️⃣ 查询优化器（Optimizer）**

- 优化器根据解析树生成 **执行计划（Execution Plan）**。
- 它会做：
  - 选择最佳索引
  - 决定表的访问顺序（表连接顺序）
  - 选择使用何种连接算法（Nested Loop Join, Hash Join 等）
  - 确定是否可以使用覆盖索引
- 可以通过 `EXPLAIN` 查看优化器选择的执行计划。

------

## ⚙️ **7️⃣ 执行器（Executor）**

- 优化器产出的执行计划，交给执行器逐步执行：
  1. 执行器调用 **存储引擎（Storage Engine）API** 获取数据（比如 InnoDB）。
  2. 如果是 `SELECT`：
     - 先看 Buffer Pool（InnoDB 缓冲池）
     - 没命中，则从磁盘读数据页，加载到缓冲池
  3. 按照索引、过滤条件、排序规则输出结果。

------

## 🔑 **8️⃣ 事务控制（对于写操作）**

- 如果是 `INSERT/UPDATE/DELETE`：
  - 执行器需要和事务系统打交道：
    - 生成 Undo Log（撤销日志）用于回滚
    - 生成 Redo Log（重做日志）用于崩溃恢复
  - 写入 Buffer Pool（内存页），并标记为脏页
  - 写入 Binlog（用于主从复制）
  - `COMMIT` 时：
    - 先刷 Redo Log（两阶段提交）
    - 然后刷 Binlog
    - 最后把脏页异步刷回磁盘（Checkpoint）。

------

## 📤 **9️⃣ 返回结果**

- `SELECT` 的结果集通过连接器返回给客户端。
- 写操作返回 OK 或错误。

------

## 🔁 **整个过程涉及的核心组件**

| 组件                | 作用                     |
| ------------------- | ------------------------ |
| 连接器              | 连接管理，认证           |
| 解析器              | 词法语法分析             |
| 优化器              | 生成高效执行计划         |
| 执行器              | 具体执行                 |
| 存储引擎（InnoDB）  | 数据实际读写，事务、索引 |
| 缓冲池              | 热数据缓存               |
| Binlog              | 主从复制                 |
| Redo Log / Undo Log | 事务一致性、崩溃恢复     |

------

## ✅ **总结版执行流程**

```
客户端 -> 连接器 -> 权限验证 -> (查询缓存) -> 解析器 -> 预处理器 -> 优化器 -> 执行器 -> 存储引擎 -> 返回结果
```

------

# 从执行流程的层面说说如何优化一条sql语句的查询速度

## 🔑 **一、SQL 执行流程回顾**

一条查询语句会经过：

```
客户端 -> 连接器 -> 权限验证 -> 解析器 -> 优化器 -> 执行器 -> 存储引擎 -> 返回结果
```

真正对 **查询性能** 有影响的主要环节是：

- 🔍 **优化器**
- ⚙️ **执行器**
- 🧩 **存储引擎**
- 🗂️ **物理存储结构（索引 / 表设计）**

------

## ⚡ **二、分阶段看怎么优化**

------

### 1️⃣ **解析器阶段（Parser）**

- 作用：词法、语法分析，生成解析树
- **影响**：这一步对性能几乎无影响，但：
  - **语法写法要正确、简洁**，避免多余嵌套，避免歧义解析。

✅ **小点：**

- 避免写 `SELECT *`，解析器需要解析所有列。
- 避免过度复杂的嵌套子查询（解析树会很深，优化器也难优化）。

------

### 2️⃣ **优化器阶段（Optimizer）**

- 作用：生成执行计划（走哪个索引、表连接顺序、是否用覆盖索引）
- **核心**：这里是 **SQL 调优最重要的一环！**

✅ **可调优点：**

| 优化项       | 如何做                                           |
| ------------ | ------------------------------------------------ |
| 索引可用性   | 创建合适的索引（B+树、覆盖索引、联合索引）       |
| 索引选择性   | 使用高区分度列做索引，避免全表扫描               |
| WHERE 子句   | 保证索引列写在 `WHERE`，避免函数对列操作（失效） |
| 排序优化     | 尽量让 `ORDER BY` 使用索引顺序扫描，避免文件排序 |
| Join 顺序    | 小表驱动大表，写法要贴合索引设计                 |
| EXPLAIN 分析 | 用 `EXPLAIN` 看是否走了预期索引                  |

🚫 常见坑：

- `SELECT *` 会让优化器必须访问所有列，覆盖索引失效。
- `OR` 条件未优化好可能导致全表扫描。

------

### 3️⃣ **执行器阶段（Executor）**

- 作用：执行器按执行计划执行，调用存储引擎。
- **瓶颈：** IO、网络传输、临时表、排序。

✅ **可调优点：**

| 优化项       | 如何做                                                       |
| ------------ | ------------------------------------------------------------ |
| 减少扫描行数 | WHERE 条件尽可能筛选小范围                                   |
| 使用 LIMIT   | 分页查询时配合索引避免全表扫描                               |
| 避免临时表   | 大量 `GROUP BY`、`ORDER BY`、`DISTINCT` 会产生临时表，尽量让它走索引 |
| 网络传输     | 只返回需要的列（避免大字段），减少结果集                     |

------

### 4️⃣ **存储引擎阶段（InnoDB）**

- 真正的物理读写发生在这里。
- 主要瓶颈是：
  - 随机 IO
  - 缓冲池命中率
  - 锁争用

✅ **可调优点：**

| 优化项    | 如何做                                             |
| --------- | -------------------------------------------------- |
| 页命中率  | 增加 Buffer Pool 大小（`innodb_buffer_pool_size`） |
| 顺序读    | 用聚簇索引减少随机读                               |
| 热点行/页 | 避免热点更新，可拆分表、分区或使用自增 ID 合理设计 |
| 大事务    | 拆小事务，避免长时间持有锁                         |

------

## ⚙️ **补充：执行计划结果怎么看？**

🔑 用 `EXPLAIN`：

- `type` 列：访问类型（`const` / `ref` / `range` / `index` / `ALL`），从高效到低效。
- `rows` 列：预估扫描行数，越少越好。
- `Extra` 列：看是否出现 `Using temporary`、`Using filesort`，一般说明有优化空间。

------

## 📌 **三、综合：写高效 SQL 的思路**

| 优化点 | 关键做法                                   |
| ------ | ------------------------------------------ |
| 设计层 | 表结构规范化、字段设计合理                 |
| 索引层 | 走对索引，覆盖索引，联合索引顺序合理       |
| 语句层 | 避免全表扫描、避免临时表、避免返回大结果集 |
| 参数层 | 合理分页、限制返回行数                     |
| 引擎层 | 合理配置缓冲池、硬件 IO 够快               |

------

## ✅ ✅ ✅ **总结一句话**

> **SQL 优化，核心就是：减少扫描行数、减少回表次数、让执行尽可能使用索引顺序读，减少临时表和排序。**

------

如果需要，我可以：

- 🗂️ 帮你分析一条具体 SQL 的 `EXPLAIN`
- ✏️ 帮你设计索引
- 🖼️ 画一张 “SQL 调优的流程图”

要不要？🚀

## 多表联合查询的时候，SQL语句的执行流程是什么样的呢？

多表联合查询时，SQL语句的执行流程在基本框架上与单表查询相似，但在某些方面会有更复杂的处理，特别是在优化器处理和执行计划实施阶段。下面是一个简化的流程，展示了多表联合查询的主要步骤：

1. **客户端发送请求**：将包含多表联合查询的SQL语句发送给MySQL服务器。

2. **语法解析和预处理**：服务器接收和解析SQL语句，检查语法错误，并对SQL进行预处理。在多表查询中，这个阶段还包括解析涉及的所有表名和列名，验证用户对这些表的访问权限等。

3. **优化器处理**：
   - **查询优化**：在多表查询中，优化器的任务更为复杂。它需要决定表之间的连接顺序（连接顺序对查询性能影响很大），选择用于每个表的索引，以及确定使用哪种连接算法（如嵌套循环连接、排序合并连接、哈希连接等）。
   - **生成多个执行计划**：优化器会考虑不同的表连接顺序和各种索引使用策略，生成多个可能的执行计划。
   - **选择最佳执行计划**：优化器通过估算每个执行计划的代价，选择最节省资源和时间的执行计划。

4. **执行计划实施**：
   - **执行连接操作**：根据优化器选定的最佳执行计划，数据库引擎开始实现表间的连接。这一步骤涉及到数据的读取、中间结果的生成和临时表的使用等。
   - **执行其他数据操作**：除了连接外，还可能执行过滤（`WHERE`子句）、分组和聚合（`GROUP BY`和聚合函数）、排序（`ORDER BY`）等操作。

5. **结果集返回**：完成所有数据处理后，数据库生成最终的查询结果集，并将这些结果返回给客户端。

在处理多表联合查询时，优化器的任务是确保查询以最有效的方式执行。正确的索引、合理的表设计和关联列类型的一致性等都会帮助优化器生成更优的执行计划。由于多表查询的复杂性，使用`EXPLAIN`
语句来分析查询的执行计划尤为重要，这有助于识别性能瓶颈和优化查询。

# EXPLAIN 关键字

MySQL的`EXPLAIN`关键字用于分析SELECT语句的执行计划，它能够告诉你MySQL是如何执行你的查询的。`EXPLAIN`会返回一组行，每一行都描述了MySQL执行查询时使用的一种查询执行计划。这些行提供了有关查询执行的详细信息，包括使用的索引、连接类型、表扫描方式等。

下面是一些`EXPLAIN`输出中常见的列及其含义：

1. `id`: 查询的序号，每一行都有一个唯一的序号，用于标识查询中不同操作的执行顺序。
2. `select_type`: 表示查询的类型，可能的值包括`SIMPLE`（简单查询）、`PRIMARY`（主查询，外层查询）、`SUBQUERY`（子查询）、`DERIVED`（派生表，临时表）、`UNION`
   （联合查询）、`UNION RESULT`（联合结果）等。
3. `table`: 显示查询涉及的表名。
4. `type`: 表示表的连接类型，包括`ALL`（全表扫描）、`index`（索引扫描）、`range`（索引范围扫描）、`ref`（基于索引的引用）、`eq_ref`（唯一索引扫描）、`const`
   （常量表）等，连接类型从最好到最差依次排列。
5. `possible_keys`: 列出可能用于查询的索引，但实际上未被选择的索引。
6. `key`: 实际用于查询的索引。
7. `key_len`: 表示索引中使用的字节数。
8. `ref`: 显示索引的列或常量与表中的哪一列进行比较。
9. `rows`: 估计查询返回的行数。
10. `Extra`: 提供关于查询执行的额外信息，例如是否使用了临时表、文件排序、使用了哪些索引等。

通过分析`EXPLAIN`输出，你可以确定查询的瓶颈所在，优化查询语句以提高性能。例如，通过确保查询使用了合适的索引、减少全表扫描等方式来改进查询执行计划。

# MySQL的零拷贝是什么?

好问题！
 **“零拷贝（Zero Copy）”** 在 MySQL（更广义是操作系统和数据库）里是一个很经典的高效 IO 技术概念，
 它的核心目的就是：**尽量减少数据在用户态和内核态之间的拷贝，提高吞吐，降低 CPU 消耗。**

------

下面我给你详细解释一下，**从底层到 MySQL 的实际用法**。

------

## ✅ 一、什么是“拷贝”？

先理解个背景：

1️⃣ 应用要读文件：

- 数据最初在磁盘（硬盘或 SSD）
- 操作系统（内核态）负责 IO
- 应用程序（用户态）需要把数据读出来，通常要从内核态到用户态有一次或多次内存拷贝。

举个常规场景：

- `read()` 系统调用：
  - 内核从磁盘读到内核缓冲区（页缓存）
  - 再把数据拷贝到用户态缓冲区（比如应用的 `recv` 缓冲区）
- `send()` 系统调用：
  - 需要把用户态的数据拷贝回内核 TCP 缓冲区

如果中间还需要落盘或者通过网络传输，通常会有**多次拷贝**。

------

## ✅ 二、什么是“零拷贝”？

**“零拷贝”** 的目标：
 **尽量减少内核态 <-> 用户态之间的内存拷贝次数，或直接在内核态完成数据传输。**

经典例子：

- **`sendfile`** 调用（Linux）
  - 内核直接把文件的数据从文件描述符拷贝到网卡缓冲区（Socket Buffer）
  - 避免了应用自己 `read` → `write` 的数据中转
- `mmap` + `write`
  - 文件映射到用户态虚拟内存，应用直接读写，避免多次 `read`/`write`。

------

## ✅ 三、MySQL 里怎么用到“零拷贝”？

**MySQL 本身不是系统调用级别的“零拷贝”实现者**，但它用到的场景是：

------

### 📌 1️⃣ 大文件备份/恢复

- 比如：
  - MySQL 物理备份工具（Percona XtraBackup、MySQL Enterprise Backup）
  - 或者用 `sendfile` 实现高效的 binlog 传输、数据文件传输
- `sendfile` 可以让备份/恢复时，直接把文件内容从磁盘传到网络，不用用户态反复拷贝。

------

### 📌 2️⃣ InnoDB Buffer Pool

- InnoDB 设计了自己的 **Buffer Pool**（缓冲池），尽量减少 OS 页缓存和 InnoDB 自己的重复缓存。
- 虽然它不是严格的“零拷贝”，但也是**减少内核态与用户态之间的重复读写**，本质是相似思路：
   👉 **尽量把热数据放在内存页里，读写直接命中，避免反复从磁盘读写。**

------

### 📌 3️⃣ 复制和日志传输

- MySQL 主从复制中，binlog 可以用 **物理传输** + 高效 IO，避免不必要的解析、转换，某些版本里底层可以走零拷贝协议（如 TCP sendfile）。

------

## ✅ 四、总结一句话

> **“零拷贝”** 是操作系统级别的高效 IO 技术，MySQL/数据库通常借助它提升 **大文件传输、备份、日志复制** 的性能，
>  同时在内部用类似思路（大页缓存、Buffer Pool）来尽量减少 CPU 与磁盘/网络之间的多次数据拷贝。

------

# MySQL 分库分表

> **单库、单表容量和并发达到瓶颈时，如何继续扩容，保证高可用和高并发。**

------

## ✅ 1️⃣ 为什么要分库分表

当你遇到以下瓶颈时，就该考虑分库分表了：

| 瓶颈           | 原因                                              |
| -------------- | ------------------------------------------------- |
| 单表数据太大   | InnoDB 单表超过亿级行，索引 B+ 树深度大，查询变慢 |
| 热点写压力过大 | 单库单表写入有单机 IO、CPU、锁的上限              |
| 并发过高       | 单机连接数、单库连接池撑不住                      |
| 备份/恢复慢    | 大表做备份、迁移，时间不可接受                    |
| 读写负载不均   | 热点表过于集中                                    |

**举例**

- 电商订单表：按天/按月可能就几亿行
- 用户表：单表可能超过千万，且读写都很频繁

------

## ✅ 2️⃣ 什么是分库分表

- **分库**：把数据按某种规则拆到多个库（一般跨物理机器），目的是把压力分散到多个 MySQL 实例。
- **分表**：把一个大表拆成多个小表（可以在同一个库，也可以跨库）。

通常是：

- **先分表 → 再分库 → 再多主多从 → 再加缓存 → 再分区**

------

## ✅ 3️⃣ 分库分表的常见策略

| 方案     | 原理                                     | 适用场景             |
| -------- | ---------------------------------------- | -------------------- |
| 垂直分库 | 按业务模块拆库，如用户库、订单库、商品库 | 模块隔离，拆单体     |
| 垂直分表 | 按表中列拆表，如冷热数据拆历史表         | 表太宽，访问模式不同 |
| 水平分库 | 按 hash/范围把表按行拆到不同库           | 单表太大，单库撑不住 |
| 水平分表 | 按 hash/范围把表拆成多张小表             | 单表太大，提升并发   |

------

### 🔑 常用路由策略

| 策略       | 示例                            |
| ---------- | ------------------------------- |
| Hash       | `user_id % 4` → 分 4 张表       |
| 范围       | 按时间段（如 2023_01, 2023_02） |
| 按地理区域 | 不同城市分不同库                |
| 自定义规则 | 如订单号带路由位                |

------

## ✅ 4️⃣ 分库分表带来的新问题

分库分表虽然性能上去了，但会带来复杂性：

| 问题     | 影响                                                         |
| -------- | ------------------------------------------------------------ |
| 跨库事务 | 单机 InnoDB 的事务是本地 ACID，分库后需要分布式事务（XA/两阶段提交/最终一致性） |
| 跨表查询 | join 变复杂，比如需要在应用层做聚合                          |
| 分页     | 分表后全局分页麻烦，需要先分表查再聚合                       |
| 路由     | 需要中间件或自己封装路由规则                                 |
| 维护成本 | 库表增多，备份/扩容/监控变难                                 |

------

## ✅ 5️⃣ 行业内怎么做

**实际中怎么落地？**

✔️ 1️⃣ 自研路由 + 中间件

> 例如：美团、滴滴、字节等公司，常用 Mycat、ShardingSphere、自研分库分表中间件。

✔️ 2️⃣ 结合缓存

> 热数据直接用 Redis/Memcached 缓存，减少跨库跨表压力。

✔️ 3️⃣ 定期归档 + 冷热分离

> 老数据归档到单独库或 HDFS，查询热数据只走在线库。

✔️ 4️⃣ 分布式数据库

> 如 OceanBase、TiDB、PolarDB-X，这些内核级支持分库分表，自动分片、路由、事务、聚合。

------

## ✅ 📌 总结一句话

> **分库分表 = 把单点 MySQL 变成分布式集群，解决单库单表的容量和并发瓶颈，但也引入了分布式一致性、路由和运维复杂性。

---

# MySQL部署架构

MySQL在实际应用中常见的部署架构，尤其是数据复制和高可用性方面的策略。这里有几种常见的MySQL部署架构：

1. **主从复制（Master-Slave Replication）**：这是最常见的MySQL部署架构之一。在这种架构中，有一个主数据库（Master）负责处理所有的写操作（INSERT, UPDATE,
   DELETE），而一个或多个从数据库（Slave）负责读操作。从数据库通过复制主数据库的数据来保持数据的同步。这种架构可以提高读取速度，实现负载均衡，并且在主服务器宕机时，从服务器可以接管，提高系统的可用性。

2. **主主复制（Master-Master Replication）**：在这种架构中，两个数据库服务器彼此都扮演对方的主和从角色，同时处理写操作。这种设置可以提高系统的容错性和可用性，但同时也增加了数据一致性维护的复杂性。

3. **读写分离（Read-Write Splitting）**
   ：读写分离通常与主从复制结合使用，可以进一步优化性能和扩展性。在这种架构中，写操作只发送到主服务器，而读操作则可以分散到一个或多个从服务器。这样可以有效地分摊查询压力，提高查询响应速度。

4. **集群（Cluster）**：例如MySQL Cluster或Galera Cluster。这类架构提供高可用性和容错能力。MySQL Cluster是基于NDB（Network
   Database）存储引擎的，为用户提供了自动分片（sharding）和同步复制。Galera Cluster则提供了基于行的同步复制，支持多主配置，允许任何节点处理写操作，所有节点都可以即时看到所有写入。

5. **分片（Sharding）**：当数据库非常庞大时，可能会采用分片技术。分片是一种数据库架构技术，将数据分布在多个数据库节点上。每个节点只存储全数据集的一部分。分片可以极大地提高大规模数据库的管理效率和查询速度。

6. **云原生架构**：随着云计算的普及，MySQL也可以部署在云环境中，如AWS RDS, Google Cloud SQL和Azure Database for
   MySQL。这些平台通常提供自管理的数据库服务，支持自动备份、复制、扩展和故障转移，使得数据库管理更简单，同时提供高可用性和弹性。

根据业务需求和运营成本的不同，企业可以选择适合自己的MySQL部署架构，以达到最佳的性能和资源利用率。